{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Koaz3guQ6B7k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from data_utils import get_examples_from_dialogues, convert_state_dict, load_dataset\n",
    "from data_utils import OntologyDSTFeature, DSTPreprocessor, _truncate_seq_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCYa_jbt6B7m"
   },
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y2Jpgv4i6B7n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_file = \"/opt/ml/repo/taepd/input/data/train_dataset/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/opt/ml/repo/taepd/input/data/train_dataset/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/opt/ml/repo/taepd/input/data/train_dataset/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vu232Prn6B7n",
    "outputId": "a17014f7-81fd-41c6-c4e6-c4958942a45e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 9300.58it/s]\n",
      "100%|██████████| 699/699 [00:00<00:00, 15738.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_examples = get_examples_from_dialogues(data=train_data,\n",
    "                                             user_first=True,\n",
    "                                             dialogue_level=True)\n",
    "\n",
    "dev_examples = get_examples_from_dialogues(data=dev_data,\n",
    "                                           user_first=True,\n",
    "                                           dialogue_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aGKZk4_n6B7o",
    "outputId": "5502faa6-a80f-496a-fe8f-399ecd62350f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1qqe5Hw36B7o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_turn = max([len(e['dialogue']) for e in train_data])\n",
    "tokenizer = BertTokenizer.from_pretrained('dsksd/bert-ko-small-minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(max_turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kq2fddAg6B7p"
   },
   "source": [
    "## TODO-1: SUMBT Preprocessor 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCoRCk1z6B7p"
   },
   "source": [
    "Ontology-based DST model인 SUMBT의 InputFeature를 만들기 위한 Preprocessor를 정의해야 합니다. <br>\n",
    "\n",
    "1. `_convert_examples_to_features` 함수의 빈칸을 매워 완성하세요.\n",
    "2. `recover_state` 함수의 빈칸을 매워 완성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p8MjYjqw6B7p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SUMBTPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=64,\n",
    "        max_turn_length=14,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.max_turn_length = max_turn_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        guid = example[0].guid.rsplit(\"-\", 1)[0]  # dialogue_idx\n",
    "        turns = []\n",
    "        token_types = []\n",
    "        labels = []\n",
    "        num_turn = None\n",
    "        for turn in example[: self.max_turn_length]:\n",
    "            assert len(turn.current_turn) == 2\n",
    "            uttrs = []\n",
    "            for segment_idx, uttr in enumerate(turn.current_turn):\n",
    "                token = self.src_tokenizer.encode(uttr, add_special_tokens=False)\n",
    "                uttrs.append(token)\n",
    "\n",
    "            _truncate_seq_pair(uttrs[0], uttrs[1], self.max_seq_length - 3)\n",
    "            tokens = (\n",
    "                [self.src_tokenizer.cls_token_id]\n",
    "                + uttrs[0]\n",
    "                + [self.src_tokenizer.sep_token_id]\n",
    "                + uttrs[1]\n",
    "                + [self.src_tokenizer.sep_token_id]\n",
    "            )\n",
    "            token_type = [0] * (len(uttrs[0]) + 2) + [1] * (len(uttrs[1]) + 1)\n",
    "            if len(tokens) < self.max_seq_length:\n",
    "                gap = self.max_seq_length - len(tokens)\n",
    "                tokens.extend([self.src_tokenizer.pad_token_id] * gap)\n",
    "                token_type.extend([0] * gap)\n",
    "            turns.append(tokens)\n",
    "            token_types.append(token_type)\n",
    "            label = []\n",
    "            if turn.label:\n",
    "                slot_dict = convert_state_dict(turn.label)\n",
    "            else:\n",
    "                slot_dict = {}\n",
    "            for slot_type in self.slot_meta:\n",
    "                value = slot_dict.get(slot_type, \"none\")\n",
    "                # TODO\n",
    "                # raise Exception('label_idx를 ontology에서 꺼내오는 코드를 작성하세요!')\n",
    "#                 label_idx = self.ontology[slot_type].index(value)  # 이렇게 해도 될듯한데?\n",
    "                if value in self.ontology[slot_type]:\n",
    "                    label_idx = self.ontology[slot_type].index(value)\n",
    "                else:\n",
    "                    label_idx = self.ontology[slot_type].index(\"none\")\n",
    "                label.append(label_idx)\n",
    "            labels.append(label)\n",
    "        num_turn = len(turns)\n",
    "        if len(turns) < self.max_turn_length:\n",
    "            gap = self.max_turn_length - len(turns)\n",
    "            for _ in range(gap):\n",
    "                dummy_turn = [self.src_tokenizer.pad_token_id] * self.max_seq_length\n",
    "                turns.append(dummy_turn)\n",
    "                token_types.append(dummy_turn)\n",
    "                dummy_label = [-1] * len(self.slot_meta)\n",
    "                labels.append(dummy_label)\n",
    "        return OntologyDSTFeature(\n",
    "            guid=guid,\n",
    "            input_ids=turns,\n",
    "            segment_ids=token_types,\n",
    "            num_turn=num_turn,\n",
    "            target_ids=labels,\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, examples))\n",
    "\n",
    "    def recover_state(self, pred_slots, num_turn):\n",
    "        states = []\n",
    "        for pred_slot in pred_slots[:num_turn]:\n",
    "            state = []\n",
    "            for s, p in zip(self.slot_meta, pred_slot):\n",
    "                v = self.ontology[s][p]\n",
    "                if v != \"none\":\n",
    "                    state.append(f\"{s}-{v}\")\n",
    "            states.append(state)\n",
    "        return states\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor([b.input_ids for b in batch])\n",
    "        segment_ids = torch.LongTensor([b.segment_ids for b in batch])\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "        target_ids = torch.LongTensor([b.target_ids for b in batch])\n",
    "        num_turns = [b.num_turn for b in batch]\n",
    "        return input_ids, segment_ids, input_masks, target_ids, num_turns, guids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhFMIiFy6B7q"
   },
   "source": [
    "## Convert_Examples_to_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EKluwCmH6B7r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = SUMBTPreprocessor(slot_meta,\n",
    "                              tokenizer,\n",
    "                              ontology=ontology,  # predefined ontology\n",
    "                              max_seq_length=64,  # 각 turn마다 최대 길이\n",
    "                              max_turn_length=max_turn)  # 각 dialogue의 최대 turn 길이\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zrIVLPMd6B7r",
    "outputId": "e2b46db4-4bc0-48b2-b23b-577fe3678543",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6301\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))  # 대화 level의 features\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowy-hat-8324:관광_식당_11\n",
      "8\n",
      "34\n",
      "64\n",
      "34\n",
      "34\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "f = train_features[0]\n",
    "\n",
    "print(f.guid)  # 대화 unique_id\n",
    "print(f.num_turn)  # 실제 대화의 turn 길이 == T\n",
    "print(len(f.input_ids))  # input_ids의 턴 길이 (max_turn_length == 현재 34)\n",
    "print(len(f.input_ids[0]))  # input_ids에서 각 턴의 최대 길이 (max_seq_length == 64)\n",
    "print(len(f.segment_ids))  # segment_ids의 턴 길이 (max_turn_length == 34)\n",
    "print(len(f.target_ids))  # target_ids의 갯수 (턴마다의 State == max_turn_length == 34)\n",
    "print(len(f.target_ids[0]))  # 각 턴마다 target의 갯수 == number of Slot Meta (== 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a3j7tb96B7r"
   },
   "source": [
    "## SUMBT 모델 선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-IjCE6Db6B7r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most of code is from https://github.com/SKTBrain/SUMBT\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CosineEmbeddingLoss, CrossEntropyLoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "\n",
    "class BertForUtteranceEncoding(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForUtteranceEncoding, self).__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        return self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scores = None\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        self.scores = scores\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "    def get_scores(self):\n",
    "        return self.scores\n",
    "\n",
    "\n",
    "class SUMBT(nn.Module):\n",
    "    def __init__(self, args, num_labels, device):  # num_labels : # of Candidate values per each Slot\n",
    "        super(SUMBT, self).__init__()\n",
    "\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.rnn_num_layers = args.num_rnn_layers\n",
    "        self.zero_init_rnn = args.zero_init_rnn\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.max_label_length = args.max_label_length\n",
    "        self.num_labels = num_labels\n",
    "        self.num_slots = len(num_labels)\n",
    "        self.attn_head = args.attn_head\n",
    "        self.device = device\n",
    "\n",
    "        ### Utterance Encoder\n",
    "        self.utterance_encoder = BertForUtteranceEncoding.from_pretrained(\n",
    "            args.model_name_or_path\n",
    "        )\n",
    "        self.bert_output_dim = self.utterance_encoder.config.hidden_size\n",
    "        self.hidden_dropout_prob = self.utterance_encoder.config.hidden_dropout_prob\n",
    "        if args.fix_utterance_encoder:\n",
    "            for p in self.utterance_encoder.bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        ### slot, slot-value Encoder (not trainable)\n",
    "        self.sv_encoder = BertForUtteranceEncoding.from_pretrained(\n",
    "            args.model_name_or_path\n",
    "        )\n",
    "        # os.path.join(args.bert_dir, 'bert-base-uncased.model'))\n",
    "        for p in self.sv_encoder.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.slot_lookup = nn.Embedding(self.num_slots, self.bert_output_dim)\n",
    "        self.value_lookup = nn.ModuleList(\n",
    "            [nn.Embedding(num_label, self.bert_output_dim) for num_label in num_labels]\n",
    "        )\n",
    "\n",
    "        ### Attention layer\n",
    "        self.attn = MultiHeadAttention(self.attn_head, self.bert_output_dim, dropout=0)\n",
    "\n",
    "        ### RNN Belief Tracker\n",
    "        self.nbt = nn.GRU(\n",
    "            input_size=self.bert_output_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.rnn_num_layers,\n",
    "            dropout=self.hidden_dropout_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.init_parameter(self.nbt)\n",
    "\n",
    "        if not self.zero_init_rnn:\n",
    "            self.rnn_init_linear = nn.Sequential(\n",
    "                nn.Linear(self.bert_output_dim, self.hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.hidden_dropout_prob),\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.bert_output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(self.bert_output_dim)\n",
    "\n",
    "        ### Measure\n",
    "#         self.metric = torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)\n",
    "        self.distance_metric = args.distance_metric\n",
    "        if self.distance_metric == \"cosine\":\n",
    "            self.metric = torch.nn.CosineSimilarity(dim=-1, eps=1e-08)\n",
    "        elif self.distance_metric == \"euclidean\":\n",
    "            self.metric = torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)\n",
    "\n",
    "        ### Classifier\n",
    "        self.nll = CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        ### Etc.\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "    def initialize_slot_value_lookup(self, label_ids, slot_ids):\n",
    "\n",
    "        self.sv_encoder.eval()\n",
    "\n",
    "        # Slot encoding\n",
    "        slot_type_ids = torch.zeros(slot_ids.size(), dtype=torch.long).to(\n",
    "            slot_ids.device\n",
    "        )\n",
    "        slot_mask = slot_ids > 0\n",
    "        hid_slot, _ = self.sv_encoder(\n",
    "            slot_ids.view(-1, self.max_label_length),\n",
    "            slot_type_ids.view(-1, self.max_label_length),\n",
    "            slot_mask.view(-1, self.max_label_length),\n",
    "        )\n",
    "        hid_slot = hid_slot[:, 0, :]\n",
    "        hid_slot = hid_slot.detach()\n",
    "        self.slot_lookup = nn.Embedding.from_pretrained(hid_slot, freeze=True)\n",
    "\n",
    "        for s, label_id in enumerate(label_ids):\n",
    "            label_type_ids = torch.zeros(label_id.size(), dtype=torch.long).to(\n",
    "                label_id.device\n",
    "            )\n",
    "            label_mask = label_id > 0\n",
    "            hid_label, _ = self.sv_encoder(\n",
    "                label_id.view(-1, self.max_label_length),\n",
    "                label_type_ids.view(-1, self.max_label_length),\n",
    "                label_mask.view(-1, self.max_label_length),\n",
    "            )\n",
    "            hid_label = hid_label[:, 0, :]\n",
    "            hid_label = hid_label.detach()\n",
    "            self.value_lookup[s] = nn.Embedding.from_pretrained(hid_label, freeze=True)\n",
    "            self.value_lookup[s].padding_idx = -1\n",
    "\n",
    "        print(\"Complete initialization of slot and value lookup\")\n",
    "        self.sv_encoder = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        token_type_ids,\n",
    "        attention_mask,\n",
    "        labels=None,\n",
    "        n_gpu=1,\n",
    "        target_slot=None,\n",
    "    ):\n",
    "        # B: Batch size, M: Max turn length, N: Max seq length, \n",
    "        # J: # of Slot Meta, H: Hidden dimension\n",
    "        \n",
    "        # input_ids: [B, M, N]\n",
    "        # token_type_ids: [B, M, N]\n",
    "        # attention_mask: [B, M, N]\n",
    "        # labels: [B, M, J]\n",
    "\n",
    "        # if target_slot is not specified, output values corresponding all slot-types\n",
    "        if target_slot is None:\n",
    "            target_slot = list(range(0, self.num_slots))\n",
    "\n",
    "        ds = input_ids.size(0)  # Batch size (B)\n",
    "        ts = input_ids.size(1)  # Max turn size (M)\n",
    "        bs = ds * ts\n",
    "        slot_dim = len(target_slot)  # J\n",
    "\n",
    "        # Utterance encoding\n",
    "        # Utterence-level로 독립적으로 인코딩하므로 flatten필요\n",
    "        hidden, _ = self.utterance_encoder(\n",
    "            input_ids.view(-1, self.max_seq_length),\n",
    "            token_type_ids.view(-1, self.max_seq_length),\n",
    "            attention_mask.view(-1, self.max_seq_length),\n",
    "        )\n",
    "        hidden = torch.mul(\n",
    "            hidden,\n",
    "            attention_mask.view(-1, self.max_seq_length, 1)\n",
    "            .expand(hidden.size())\n",
    "            .float(),\n",
    "        )\n",
    "        hidden = hidden.repeat(slot_dim, 1, 1)  # [J*M*B, N, H]\n",
    "\n",
    "        hid_slot = self.slot_lookup.weight[target_slot, :]  # Select target slot embedding\n",
    "        hid_slot = hid_slot.repeat(1, bs).view(bs * slot_dim, -1)  # [J*M*B, N, H]\n",
    "\n",
    "        # Attended utterance vector\n",
    "        hidden = self.attn(\n",
    "            hid_slot,  # q^s  [J*M*B, N, H]\n",
    "            hidden,  # U [J*M*B, N, H]\n",
    "            hidden,  # U [J*M*B, N, H]\n",
    "            mask=attention_mask.view(-1, 1, self.max_seq_length).repeat(slot_dim, 1, 1),\n",
    "        )\n",
    "        hidden = hidden.squeeze()  # h [J*M*B, H] Aggregated Slot Context\n",
    "        hidden = hidden.view(slot_dim, ds, ts, -1).view(-1, ts, self.bert_output_dim)  # [J*B, M, H]\n",
    "\n",
    "        # NBT\n",
    "        if self.zero_init_rnn:\n",
    "            h = torch.zeros(\n",
    "                self.rnn_num_layers, input_ids.shape[0] * slot_dim, self.hidden_dim\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [1, slot_dim*ds, hidden]\n",
    "        else:\n",
    "            h = hidden[:, 0, :].unsqueeze(0).repeat(self.rnn_num_layers, 1, 1)\n",
    "            h = self.rnn_init_linear(h)\n",
    "\n",
    "        if isinstance(self.nbt, nn.GRU):\n",
    "            rnn_out, _ = self.nbt(hidden, h)  # [J*B, M, H_GRU]\n",
    "        elif isinstance(self.nbt, nn.LSTM):\n",
    "            c = torch.zeros(\n",
    "                self.rnn_num_layers, input_ids.shape[0] * slot_dim, self.hidden_dim\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [1, slot_dim*ds, hidden]\n",
    "            rnn_out, _ = self.nbt(hidden, (h, c))  # [slot_dim*ds, turn, hidden]\n",
    "        rnn_out = self.layer_norm(self.linear(self.dropout(rnn_out)))\n",
    "\n",
    "        hidden = rnn_out.view(slot_dim, ds, ts, -1)  # [J, B, M, H_GRU]\n",
    "\n",
    "        # Label (slot-value) encoding\n",
    "        loss = 0\n",
    "        loss_slot = []\n",
    "        pred_slot = []\n",
    "        output = []\n",
    "        for s, slot_id in enumerate(target_slot):  ## note: target_slots are successive\n",
    "            # loss calculation\n",
    "            hid_label = self.value_lookup[slot_id].weight\n",
    "            num_slot_labels = hid_label.size(0)\n",
    "\n",
    "            _hid_label = (\n",
    "                hid_label.unsqueeze(0)\n",
    "                .unsqueeze(0)\n",
    "                .repeat(ds, ts, 1, 1)\n",
    "                .view(ds * ts * num_slot_labels, -1)\n",
    "            )\n",
    "            _hidden = (\n",
    "                hidden[s, :, :, :]\n",
    "                .unsqueeze(2)\n",
    "                .repeat(1, 1, num_slot_labels, 1)\n",
    "                .view(ds * ts * num_slot_labels, -1)\n",
    "            )\n",
    "            _dist = self.metric(_hid_label, _hidden).view(ds, ts, num_slot_labels)\n",
    "            if self.distance_metric == \"euclidean\":\n",
    "                _dist = -_dist\n",
    "            _, pred = torch.max(_dist, -1)  # taget_ids에서 ignore index 즉, padding일 경우 -1로 setting했었음\n",
    "            pred_slot.append(pred.view(ds, ts, 1))\n",
    "            output.append(_dist)\n",
    "\n",
    "            if labels is not None:\n",
    "                _loss = self.nll(_dist.view(ds * ts, -1), labels[:, :, s].view(-1))\n",
    "                loss_slot.append(_loss.item())\n",
    "                loss += _loss\n",
    "\n",
    "        pred_slot = torch.cat(pred_slot, 2)\n",
    "        if labels is None:\n",
    "            return output, pred_slot\n",
    "\n",
    "        # calculate joint accuracy\n",
    "        accuracy = (pred_slot == labels).view(-1, slot_dim)\n",
    "        acc_slot = (\n",
    "            torch.sum(accuracy, 0).float()\n",
    "            / torch.sum(labels.view(-1, slot_dim) > -1, 0).float()\n",
    "        )\n",
    "        acc = (\n",
    "            sum(torch.sum(accuracy, 1) / slot_dim).float()\n",
    "            / torch.sum(labels[:, :, 0].view(-1) > -1, 0).float()\n",
    "        )  # joint accuracy\n",
    "\n",
    "        if n_gpu == 1:\n",
    "            return loss, loss_slot, acc, acc_slot, pred_slot\n",
    "        else:\n",
    "            return (\n",
    "                loss.unsqueeze(0),\n",
    "                None,\n",
    "                acc.unsqueeze(0),\n",
    "                acc_slot.unsqueeze(0),\n",
    "                pred_slot.unsqueeze(0),\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def init_parameter(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(module.weight)\n",
    "            torch.nn.init.constant_(module.bias, 0.0)\n",
    "        elif isinstance(module, nn.GRU) or isinstance(module, nn.LSTM):\n",
    "            torch.nn.init.xavier_normal_(module.weight_ih_l0)\n",
    "            torch.nn.init.xavier_normal_(module.weight_hh_l0)\n",
    "            torch.nn.init.constant_(module.bias_ih_l0, 0.0)\n",
    "            torch.nn.init.constant_(module.bias_hh_l0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYm56GEZ6B7u"
   },
   "source": [
    "## TODO-2: Ontology Pre-Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WdPyqp16B7w"
   },
   "source": [
    "Ontology의 slot type들과 이에 속하는 slot_value들을 tokenizing하는 `tokenize_ontology`를 작성하세요. <br>\n",
    "[CLS] Pooling하여 `slot_lookup` 과 `value_lookup` embedding matrix들을 초기화하는 <br>\n",
    "`initialize_slot_value_lookup`에 인자로 넘겨주세요. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zwgNUZNn6B7w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_ontology(ontology, tokenizer, max_seq_length=12):\n",
    "    slot_types = []\n",
    "    slot_values = []\n",
    "    for k, v in ontology.items():\n",
    "        tokens = tokenizer.encode(k)\n",
    "        if len(tokens) < max_seq_length:\n",
    "            gap = max_seq_length - len(tokens)\n",
    "            tokens.extend([tokenizer.pad_token_id] *  gap)\n",
    "        slot_types.append(tokens)\n",
    "        slot_value = []\n",
    "        for vv in v:\n",
    "            tokens = tokenizer.encode(vv)\n",
    "            if len(tokens) < max_seq_length:\n",
    "                gap = max_seq_length - len(tokens)\n",
    "                tokens.extend([tokenizer.pad_token_id] *  gap)\n",
    "            slot_value.append(tokens)\n",
    "        slot_values.append(torch.LongTensor(slot_value))\n",
    "    return torch.LongTensor(slot_types), slot_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "attmAW3A6B7w",
    "outputId": "9dbf4e3d-f077-4782-9d3b-d7c8c00159ff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 103, 13, 4, 7, 5, 4, 4, 4, 12, 12, 9, 67, 4, 4, 7, 4, 7, 4, 4, 5, 4, 4, 12, 569, 9, 44, 4, 10, 4, 4, 7, 4, 60, 12, 60, 190, 298, 5, 431, 286]\n",
      "Tokenized Slot:  torch.Size([45, 12])\n",
      "Tokenized Value of 관광-경치 좋은 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-교육적 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-도보 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-문화 예술 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-역사적 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-이름 torch.Size([103, 12])\n",
      "Tokenized Value of 관광-종류 torch.Size([13, 12])\n",
      "Tokenized Value of 관광-주차 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-지역 torch.Size([7, 12])\n",
      "Tokenized Value of 숙소-가격대 torch.Size([5, 12])\n",
      "Tokenized Value of 숙소-도보 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-수영장 유무 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-스파 유무 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-예약 기간 torch.Size([12, 12])\n",
      "Tokenized Value of 숙소-예약 명수 torch.Size([12, 12])\n",
      "Tokenized Value of 숙소-예약 요일 torch.Size([9, 12])\n",
      "Tokenized Value of 숙소-이름 torch.Size([67, 12])\n",
      "Tokenized Value of 숙소-인터넷 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-조식 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-종류 torch.Size([7, 12])\n",
      "Tokenized Value of 숙소-주차 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-지역 torch.Size([7, 12])\n",
      "Tokenized Value of 숙소-헬스장 유무 torch.Size([4, 12])\n",
      "Tokenized Value of 숙소-흡연 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-가격대 torch.Size([5, 12])\n",
      "Tokenized Value of 식당-도보 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-야외석 유무 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-예약 명수 torch.Size([12, 12])\n",
      "Tokenized Value of 식당-예약 시간 torch.Size([569, 12])\n",
      "Tokenized Value of 식당-예약 요일 torch.Size([9, 12])\n",
      "Tokenized Value of 식당-이름 torch.Size([44, 12])\n",
      "Tokenized Value of 식당-인터넷 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-종류 torch.Size([10, 12])\n",
      "Tokenized Value of 식당-주류 판매 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-주차 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 식당-지역 torch.Size([7, 12])\n",
      "Tokenized Value of 식당-흡연 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 지하철-도착지 torch.Size([60, 12])\n",
      "Tokenized Value of 지하철-출발 시간 torch.Size([12, 12])\n",
      "Tokenized Value of 지하철-출발지 torch.Size([60, 12])\n",
      "Tokenized Value of 택시-도착 시간 torch.Size([190, 12])\n",
      "Tokenized Value of 택시-도착지 torch.Size([298, 12])\n",
      "Tokenized Value of 택시-종류 torch.Size([5, 12])\n",
      "Tokenized Value of 택시-출발 시간 torch.Size([431, 12])\n",
      "Tokenized Value of 택시-출발지 torch.Size([286, 12])\n"
     ]
    }
   ],
   "source": [
    "slot_type_ids, slot_values_ids = tokenize_ontology(ontology, tokenizer, 12)\n",
    "num_labels = [len(s) for s in slot_values_ids]  # 각 Slot 별 후보 Values의 갯수\n",
    "print(num_labels)\n",
    "print(\"Tokenized Slot: \", slot_type_ids.size())\n",
    "for slot, slot_value_id in zip(slot_meta, slot_values_ids):\n",
    "    print(f\"Tokenized Value of {slot}\", slot_value_id.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCsCGN0f6B7x"
   },
   "source": [
    "## Model 선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aQSRYG6e6B7x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = {\n",
    "    'hidden_dim': 300,\n",
    "    'num_rnn_layers': 1,\n",
    "    'zero_init_rnn': False,\n",
    "    'max_seq_length': 64,\n",
    "    'max_label_length': 12,\n",
    "    'attn_head': 4,\n",
    "    'fix_utterance_encoder': False,\n",
    "    'task_name': 'sumbtgru',\n",
    "    'distance_metric': 'euclidean',\n",
    "    'model_name_or_path': 'dsksd/bert-ko-small-minimal',\n",
    "    'warmup_ratio': 0.1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_train_epochs': 20\n",
    "}\n",
    "\n",
    "args = Namespace(**args)\n",
    "\n",
    "num_labels = [len(s) for s in slot_values_ids]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = 1 if torch.cuda.device_count() < 2 else torch.cuda.device_count()\n",
    "n_epochs = args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NZNUF9k86B7x",
    "outputId": "4ef347fd-6fa1-4f26-e8a4-8133614f9380",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dsksd/bert-ko-small-minimal were not used when initializing BertForUtteranceEncoding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dsksd/bert-ko-small-minimal were not used when initializing BertForUtteranceEncoding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initialization of slot and value lookup\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SUMBT(args, num_labels, device)\n",
    "model.initialize_slot_value_lookup(slot_values_ids, slot_type_ids)  # Tokenized Ontology의 Pre-encoding using BERT_SV\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulJJGp5a6B7x"
   },
   "source": [
    "## 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yBU7KP5A6B7y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_utils import WOSDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import random\n",
    "\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=12, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVQC_XUG6B7y"
   },
   "source": [
    "## Optimizer & Scheduler 선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wK-LWnHU6B7y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(t_total * args.warmup_ratio), num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-f0L44h6B7y"
   },
   "source": [
    "## TODO-3: Inference code 작성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jCGuenx36B7y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluation import _evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LBXWTFHi6B7y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, eval_loader, processor, device):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    for batch in tqdm(eval_loader):\n",
    "        input_ids, segment_ids, input_masks, target_ids, num_turns, guids = \\\n",
    "        [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, pred_slot = model(\n",
    "                input_ids, segment_ids, input_masks, labels=None, n_gpu=1\n",
    "            )\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        for i in range(batch_size):\n",
    "            guid = guids[i]\n",
    "            states = processor.recover_state(pred_slot.tolist()[i], num_turns[i])\n",
    "            for tid, state in enumerate(states):\n",
    "                predictions[f\"{guid}-{tid}\"] = state\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW-qyUd_6B7y"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S7tpx4426B7z",
    "outputId": "9538d5cc-c271-4176-e1a1-af2d1121c40c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20] [0/526] 114.868095\n",
      "[0/20] [100/526] 43.948715\n",
      "[0/20] [200/526] 41.358509\n",
      "[0/20] [300/526] 40.092545\n",
      "[0/20] [400/526] 29.480150\n",
      "[0/20] [500/526] 27.281750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.02321857485988791, 'turn_slot_accuracy': 0.8429454674850964, 'turn_slot_f1': 0.1871655046778082}\n",
      "joint_goal_accuracy: 0.02321857485988791\n",
      "turn_slot_accuracy: 0.8429454674850964\n",
      "turn_slot_f1: 0.1871655046778082\n",
      "[1/20] [0/526] 30.129681\n",
      "[1/20] [100/526] 25.904070\n",
      "[1/20] [200/526] 23.002653\n",
      "[1/20] [300/526] 17.954689\n",
      "[1/20] [400/526] 15.841918\n",
      "[1/20] [500/526] 16.558887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.08126501200960769, 'turn_slot_accuracy': 0.8913174984431995, 'turn_slot_f1': 0.5105466361934005}\n",
      "joint_goal_accuracy: 0.08126501200960769\n",
      "turn_slot_accuracy: 0.8913174984431995\n",
      "turn_slot_f1: 0.5105466361934005\n",
      "[2/20] [0/526] 23.267542\n",
      "[2/20] [100/526] 13.109900\n",
      "[2/20] [200/526] 12.278832\n",
      "[2/20] [300/526] 10.784576\n",
      "[2/20] [400/526] 12.519567\n",
      "[2/20] [500/526] 9.103028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.35588470776621295, 'turn_slot_accuracy': 0.970056044835877, 'turn_slot_f1': 0.865562873749447}\n",
      "joint_goal_accuracy: 0.35588470776621295\n",
      "turn_slot_accuracy: 0.970056044835877\n",
      "turn_slot_f1: 0.865562873749447\n",
      "[3/20] [0/526] 7.468665\n",
      "[3/20] [100/526] 7.146386\n",
      "[3/20] [200/526] 5.792845\n",
      "[3/20] [300/526] 6.277837\n",
      "[3/20] [400/526] 4.851289\n",
      "[3/20] [500/526] 5.828955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.5374299439551641, 'turn_slot_accuracy': 0.9822391246330479, 'turn_slot_f1': 0.9229680662353011}\n",
      "joint_goal_accuracy: 0.5374299439551641\n",
      "turn_slot_accuracy: 0.9822391246330479\n",
      "turn_slot_f1: 0.9229680662353011\n",
      "[4/20] [0/526] 3.817214\n",
      "[4/20] [100/526] 4.795888\n",
      "[4/20] [200/526] 3.374427\n",
      "[4/20] [300/526] 4.124281\n",
      "[4/20] [400/526] 4.928007\n",
      "[4/20] [500/526] 2.409873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.6985588470776621, 'turn_slot_accuracy': 0.9903255938083848, 'turn_slot_f1': 0.9621004816567196}\n",
      "joint_goal_accuracy: 0.6985588470776621\n",
      "turn_slot_accuracy: 0.9903255938083848\n",
      "turn_slot_f1: 0.9621004816567196\n",
      "[5/20] [0/526] 2.822096\n",
      "[5/20] [100/526] 3.736458\n",
      "[5/20] [200/526] 3.471857\n",
      "[5/20] [300/526] 3.390440\n",
      "[5/20] [400/526] 2.067962\n",
      "[5/20] [500/526] 1.630800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.7397918334667735, 'turn_slot_accuracy': 0.9916066186282385, 'turn_slot_f1': 0.9687122360807113}\n",
      "joint_goal_accuracy: 0.7397918334667735\n",
      "turn_slot_accuracy: 0.9916066186282385\n",
      "turn_slot_f1: 0.9687122360807113\n",
      "[6/20] [0/526] 3.130288\n",
      "[6/20] [100/526] 2.258216\n",
      "[6/20] [200/526] 1.917118\n",
      "[6/20] [300/526] 1.277083\n",
      "[6/20] [400/526] 1.889010\n",
      "[6/20] [500/526] 1.908864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.7748198558847078, 'turn_slot_accuracy': 0.9931500756160516, 'turn_slot_f1': 0.9736559247159359}\n",
      "joint_goal_accuracy: 0.7748198558847078\n",
      "turn_slot_accuracy: 0.9931500756160516\n",
      "turn_slot_f1: 0.9736559247159359\n",
      "[7/20] [0/526] 2.062464\n",
      "[7/20] [100/526] 1.569156\n",
      "[7/20] [200/526] 2.965458\n",
      "[7/20] [300/526] 1.190115\n",
      "[7/20] [400/526] 2.255573\n",
      "[7/20] [500/526] 1.742431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.7718174539631706, 'turn_slot_accuracy': 0.9930878035761973, 'turn_slot_f1': 0.9743543711447822}\n",
      "joint_goal_accuracy: 0.7718174539631706\n",
      "turn_slot_accuracy: 0.9930878035761973\n",
      "turn_slot_f1: 0.9743543711447822\n",
      "[8/20] [0/526] 1.254343\n",
      "[8/20] [100/526] 1.784663\n",
      "[8/20] [200/526] 1.804575\n",
      "[8/20] [300/526] 1.601935\n",
      "[8/20] [400/526] 2.214295\n",
      "[8/20] [500/526] 1.406150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.7770216172938351, 'turn_slot_accuracy': 0.9933013077128398, 'turn_slot_f1': 0.9742607791585527}\n",
      "joint_goal_accuracy: 0.7770216172938351\n",
      "turn_slot_accuracy: 0.9933013077128398\n",
      "turn_slot_f1: 0.9742607791585527\n",
      "[9/20] [0/526] 1.135330\n",
      "[9/20] [100/526] 1.478553\n",
      "[9/20] [200/526] 1.501006\n",
      "[9/20] [300/526] 1.077645\n",
      "[9/20] [400/526] 1.287126\n",
      "[9/20] [500/526] 1.953244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8000400320256205, 'turn_slot_accuracy': 0.9940619161996301, 'turn_slot_f1': 0.9772465170114577}\n",
      "joint_goal_accuracy: 0.8000400320256205\n",
      "turn_slot_accuracy: 0.9940619161996301\n",
      "turn_slot_f1: 0.9772465170114577\n",
      "[10/20] [0/526] 1.753808\n",
      "[10/20] [100/526] 1.504511\n",
      "[10/20] [200/526] 0.983572\n",
      "[10/20] [300/526] 1.817090\n",
      "[10/20] [400/526] 1.534992\n",
      "[10/20] [500/526] 0.886715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8038430744595677, 'turn_slot_accuracy': 0.9939284761142267, 'turn_slot_f1': 0.9778885408943266}\n",
      "joint_goal_accuracy: 0.8038430744595677\n",
      "turn_slot_accuracy: 0.9939284761142267\n",
      "turn_slot_f1: 0.9778885408943266\n",
      "[11/20] [0/526] 1.054771\n",
      "[11/20] [100/526] 1.184391\n",
      "[11/20] [200/526] 1.965092\n",
      "[11/20] [300/526] 1.285452\n",
      "[11/20] [400/526] 1.328396\n",
      "[11/20] [500/526] 1.136466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.811048839071257, 'turn_slot_accuracy': 0.9944488924472933, 'turn_slot_f1': 0.9792614352618286}\n",
      "joint_goal_accuracy: 0.811048839071257\n",
      "turn_slot_accuracy: 0.9944488924472933\n",
      "turn_slot_f1: 0.9792614352618286\n",
      "[12/20] [0/526] 0.895178\n",
      "[12/20] [100/526] 1.027833\n",
      "[12/20] [200/526] 1.016667\n",
      "[12/20] [300/526] 1.657724\n",
      "[12/20] [400/526] 0.796924\n",
      "[12/20] [500/526] 1.045228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8108486789431545, 'turn_slot_accuracy': 0.9943688283960548, 'turn_slot_f1': 0.9795099021446407}\n",
      "joint_goal_accuracy: 0.8108486789431545\n",
      "turn_slot_accuracy: 0.9943688283960548\n",
      "turn_slot_f1: 0.9795099021446407\n",
      "[13/20] [0/526] 1.347150\n",
      "[13/20] [100/526] 0.754413\n",
      "[13/20] [200/526] 0.717257\n",
      "[13/20] [300/526] 0.937202\n",
      "[13/20] [400/526] 0.926907\n",
      "[13/20] [500/526] 0.695811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8166533226581265, 'turn_slot_accuracy': 0.9944622364558348, 'turn_slot_f1': 0.9794021951444372}\n",
      "joint_goal_accuracy: 0.8166533226581265\n",
      "turn_slot_accuracy: 0.9944622364558348\n",
      "turn_slot_f1: 0.9794021951444372\n",
      "[14/20] [0/526] 0.648972\n",
      "[14/20] [100/526] 0.810817\n",
      "[14/20] [200/526] 0.787708\n",
      "[14/20] [300/526] 0.588056\n",
      "[14/20] [400/526] 0.813176\n",
      "[14/20] [500/526] 1.834406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8174539631705364, 'turn_slot_accuracy': 0.994493372475761, 'turn_slot_f1': 0.9789641614219999}\n",
      "joint_goal_accuracy: 0.8174539631705364\n",
      "turn_slot_accuracy: 0.994493372475761\n",
      "turn_slot_f1: 0.9789641614219999\n",
      "[15/20] [0/526] 0.765076\n",
      "[15/20] [100/526] 0.945455\n",
      "[15/20] [200/526] 0.918456\n",
      "[15/20] [300/526] 1.069535\n",
      "[15/20] [400/526] 0.734280\n",
      "[15/20] [500/526] 0.565957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8210568454763811, 'turn_slot_accuracy': 0.9947024286095573, 'turn_slot_f1': 0.9802989302641117}\n",
      "joint_goal_accuracy: 0.8210568454763811\n",
      "turn_slot_accuracy: 0.9947024286095573\n",
      "turn_slot_f1: 0.9802989302641117\n",
      "[16/20] [0/526] 0.633327\n",
      "[16/20] [100/526] 0.793010\n",
      "[16/20] [200/526] 0.746782\n",
      "[16/20] [300/526] 1.038862\n",
      "[16/20] [400/526] 0.545648\n",
      "[16/20] [500/526] 0.844393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8158526821457166, 'turn_slot_accuracy': 0.9945600925184622, 'turn_slot_f1': 0.9797262925031408}\n",
      "joint_goal_accuracy: 0.8158526821457166\n",
      "turn_slot_accuracy: 0.9945600925184622\n",
      "turn_slot_f1: 0.9797262925031408\n",
      "[17/20] [0/526] 0.760549\n",
      "[17/20] [100/526] 0.801691\n",
      "[17/20] [200/526] 0.708253\n",
      "[17/20] [300/526] 0.657515\n",
      "[17/20] [400/526] 0.725682\n",
      "[17/20] [500/526] 0.736789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8186549239391513, 'turn_slot_accuracy': 0.9945645405213086, 'turn_slot_f1': 0.9797149571169287}\n",
      "joint_goal_accuracy: 0.8186549239391513\n",
      "turn_slot_accuracy: 0.9945645405213086\n",
      "turn_slot_f1: 0.9797149571169287\n",
      "[18/20] [0/526] 0.815871\n",
      "[18/20] [100/526] 0.773245\n",
      "[18/20] [200/526] 0.651692\n",
      "[18/20] [300/526] 0.818184\n",
      "[18/20] [400/526] 0.891035\n",
      "[18/20] [500/526] 0.542635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.821857485988791, 'turn_slot_accuracy': 0.9946801885953231, 'turn_slot_f1': 0.9800905216698274}\n",
      "joint_goal_accuracy: 0.821857485988791\n",
      "turn_slot_accuracy: 0.9946801885953231\n",
      "turn_slot_f1: 0.9800905216698274\n",
      "[19/20] [0/526] 0.616937\n",
      "[19/20] [100/526] 0.827886\n",
      "[19/20] [200/526] 0.388662\n",
      "[19/20] [300/526] 0.606232\n",
      "[19/20] [400/526] 0.718389\n",
      "[19/20] [500/526] 0.616348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:30<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.8202562049639712, 'turn_slot_accuracy': 0.9946312605640099, 'turn_slot_f1': 0.9799931598939937}\n",
      "joint_goal_accuracy: 0.8202562049639712\n",
      "turn_slot_accuracy: 0.9946312605640099\n",
      "turn_slot_f1: 0.9799931598939937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_score, best_checkpoint = 0, 0\n",
    "for epoch in range(n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, target_ids, num_turns, guids  = \\\n",
    "        [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "\n",
    "        # Forward\n",
    "        if n_gpu == 1:\n",
    "            loss, loss_slot, acc, acc_slot, _ = model(input_ids, segment_ids, input_masks, target_ids, n_gpu)\n",
    "        else:\n",
    "            loss, _, acc, acc_slot, _ = model(input_ids, segment_ids, input_masks, target_ids, n_gpu)\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "            \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHUyY11B6B70"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VzusVESG6B70",
    "outputId": "972f9d9f-336e-452d-c535-0aef4638f40f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 16140.54it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/repo/taepd/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=True, dialogue_level=True\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "88F6j_5v6B71",
    "outputId": "9fdd691f-2974-47d4-f71c-fd454048f9eb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:25<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1-IQhzx16B71"
   },
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SUMBT.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
