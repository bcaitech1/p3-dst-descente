{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab8be6b-35b0-47d8-b3b6-da36e430e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb758a5a",
   "metadata": {
    "id": "eb758a5a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (BertTokenizer, \n",
    "                          BertModel,  \n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup, \n",
    "                          BertConfig)\n",
    "from transformers.modeling_bert import BertOnlyMLMHead\n",
    "from data_utils import (load_dataset, \n",
    "                        get_examples_from_dialogues, \n",
    "                        convert_state_dict, \n",
    "                        DSTInputExample, \n",
    "                        OpenVocabDSTFeature, \n",
    "                        DSTPreprocessor, \n",
    "                        WOSDataset)\n",
    "\n",
    "from inference import inference\n",
    "from evaluation import _evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc29014",
   "metadata": {
    "id": "1dc29014",
    "outputId": "d08adfe1-de56-4c74-f44f-336d972a14fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 8709.63it/s] \n",
      "100%|██████████| 699/699 [00:00<00:00, 15282.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_file = \"/opt/ml/repo/taepd/input/data/train_dataset/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/opt/ml/repo/taepd/input/data/train_dataset/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/opt/ml/repo/taepd/input/data/train_dataset/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)\n",
    "\n",
    "# dialogue_level=False : SUMBT와 다르게 dialogue context level로 input하므로\n",
    "train_examples = get_examples_from_dialogues(train_data,\n",
    "                                             user_first=False,\n",
    "                                             dialogue_level=False)\n",
    "dev_examples = get_examples_from_dialogues(dev_data,\n",
    "                                           user_first=False,\n",
    "                                           dialogue_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cddce0f",
   "metadata": {
    "id": "3cddce0f",
    "outputId": "d7bf48e6-3f20-4688-dad6-572b818950aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46315\n",
      "4930\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))\n",
    "print(len(dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee874d10",
   "metadata": {
    "id": "ee874d10"
   },
   "source": [
    "## TRADE Preprocessor\n",
    "\n",
    "BERT Encoder가 적용된 TRADE의 preprocessor입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3af720",
   "metadata": {
    "id": "6a3af720"
   },
   "outputs": [],
   "source": [
    "class TRADEPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=512,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.gating2id = {\"none\": 0, \"dontcare\": 1, \"yes\": 2, \"no\": 3, \"ptr\": 4}\n",
    "        self.id2gating = {v: k for k, v in self.gating2id.items()}\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
    "\n",
    "        input_id = self.src_tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
    "        max_length = self.max_seq_length - 2\n",
    "        if len(input_id) > max_length:\n",
    "            gap = len(input_id) - max_length\n",
    "            input_id = input_id[gap:]\n",
    "\n",
    "        input_id = (\n",
    "            [self.src_tokenizer.cls_token_id]\n",
    "            + input_id\n",
    "            + [self.src_tokenizer.sep_token_id]\n",
    "        )\n",
    "        segment_id = [0] * len(input_id)\n",
    "\n",
    "        target_ids = []\n",
    "        gating_id = []\n",
    "        if not example.label:\n",
    "            example.label = []\n",
    "\n",
    "        state = convert_state_dict(example.label)\n",
    "        for slot in self.slot_meta:\n",
    "            value = state.get(slot, \"none\")\n",
    "            target_id = self.trg_tokenizer.encode(value, add_special_tokens=False) + [\n",
    "                self.trg_tokenizer.sep_token_id\n",
    "            ]\n",
    "            target_ids.append(target_id)\n",
    "            gating_id.append(self.gating2id.get(value, self.gating2id[\"ptr\"]))\n",
    "        target_ids = self.pad_ids(target_ids, self.trg_tokenizer.pad_token_id)\n",
    "        return OpenVocabDSTFeature(\n",
    "            example.guid, input_id, segment_id, gating_id, target_ids\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, examples))\n",
    "\n",
    "    def recover_state(self, gate_list, gen_list):\n",
    "        assert len(gate_list) == len(self.slot_meta)\n",
    "        assert len(gen_list) == len(self.slot_meta)\n",
    "\n",
    "        recovered = []\n",
    "        for slot, gate, value in zip(self.slot_meta, gate_list, gen_list):\n",
    "            if self.id2gating[gate] == \"none\":\n",
    "                continue\n",
    "\n",
    "            if self.id2gating[gate] in [\"dontcare\", \"yes\", \"no\"]:\n",
    "                recovered.append(\"%s-%s\" % (slot, self.id2gating[gate]))\n",
    "                continue\n",
    "\n",
    "            token_id_list = []\n",
    "            for id_ in value:\n",
    "                if id_ in self.trg_tokenizer.all_special_ids:\n",
    "                    break\n",
    "\n",
    "                token_id_list.append(id_)\n",
    "            value = self.trg_tokenizer.decode(token_id_list, skip_special_tokens=True)\n",
    "\n",
    "            if value == \"none\":\n",
    "                continue\n",
    "\n",
    "            recovered.append(\"%s-%s\" % (slot, value))\n",
    "        return recovered\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.input_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        segment_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.segment_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "\n",
    "        gating_ids = torch.LongTensor([b.gating_id for b in batch])\n",
    "        target_ids = self.pad_id_of_matrix(\n",
    "            [torch.LongTensor(b.target_ids) for b in batch],\n",
    "            self.trg_tokenizer.pad_token_id,\n",
    "        )\n",
    "        return input_ids, segment_ids, input_masks, gating_ids, target_ids, guids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d083031",
   "metadata": {
    "id": "1d083031"
   },
   "source": [
    "## Convert_Examples_to_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a534ee7",
   "metadata": {
    "id": "9a534ee7",
    "outputId": "986287a8-eed6-47e7-fb82-79a272e59914"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dsksd/bert-ko-small-minimal')\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512)\n",
    "\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12fe89e6",
   "metadata": {
    "id": "12fe89e6",
    "outputId": "04cd16ee-428b-4ca2-ed05-e341582028fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46315\n",
      "4930\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09aa88",
   "metadata": {
    "id": "6e09aa88"
   },
   "source": [
    "# TRADE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba9a4a",
   "metadata": {
    "id": "4eba9a4a"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb123f9",
   "metadata": {
    "id": "3cb123f9"
   },
   "outputs": [],
   "source": [
    "class TRADE(nn.Module):\n",
    "    def __init__(self, config, slot_vocab, slot_meta, pad_idx=0):\n",
    "        super(TRADE, self).__init__()\n",
    "        self.config = config\n",
    "        self.slot_meta = slot_meta\n",
    "        if config.model_name_or_path:\n",
    "            self.encoder = BertModel.from_pretrained(config.model_name_or_path)\n",
    "        else:\n",
    "            self.encoder = BertModel(config)\n",
    "            \n",
    "        self.decoder = SlotGenerator(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            config.hidden_dropout_prob,\n",
    "            config.n_gate,\n",
    "            None,\n",
    "            pad_idx,\n",
    "        )\n",
    "        \n",
    "        self.decoder.set_slot_idx(slot_vocab)\n",
    "        \n",
    "        self.mlm_head = BertOnlyMLMHead(config)\n",
    "        self.tie_weight()\n",
    "\n",
    "    def tie_weight(self):\n",
    "        self.decoder.embed.weight = self.encoder.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask=None, max_len=10, teacher=None):\n",
    "\n",
    "        encoder_outputs, pooled_output = self.encoder(input_ids=input_ids)\n",
    "        all_point_outputs, all_gate_outputs = self.decoder(\n",
    "            input_ids, encoder_outputs, pooled_output.unsqueeze(0), attention_mask, max_len, teacher\n",
    "        )\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_tokens(inputs, tokenizer, config, mlm_probability=0.15):\n",
    "        \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "        probability_matrix = torch.full(labels.shape, mlm_probability).to(device)\n",
    "        #special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
    "\n",
    "        probability_matrix.masked_fill_(torch.eq(labels, 0), value=0.0)\n",
    "\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).to(device=device, dtype=torch.bool) & masked_indices\n",
    "        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).to(device=device, dtype=torch.bool) & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(config.vocab_size, labels.shape, device=device, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random].to(device)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    \n",
    "    def forward_pretrain(self, input_ids, tokenizer):\n",
    "        input_ids, labels = self.mask_tokens(input_ids, tokenizer, self.config)\n",
    "        encoder_outputs, _ = self.encoder(input_ids=input_ids)\n",
    "        mlm_logits = self.mlm_head(encoder_outputs)\n",
    "        \n",
    "        return mlm_logits, labels\n",
    "    \n",
    "class SlotGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
    "    ):\n",
    "        super(SlotGenerator, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_idx\n",
    "        )  # shared with encoder\n",
    "\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.n_gate = n_gate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
    "\n",
    "    def set_slot_idx(self, slot_vocab_idx):\n",
    "        whole = []\n",
    "        max_length = max(map(len, slot_vocab_idx))\n",
    "        for idx in slot_vocab_idx:\n",
    "            if len(idx) < max_length:\n",
    "                gap = max_length - len(idx)\n",
    "                idx.extend([self.pad_idx] * gap)\n",
    "            whole.append(idx)\n",
    "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
    "    ):\n",
    "        input_masks = input_masks.ne(1)\n",
    "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
    "        # J,2\n",
    "        batch_size = encoder_output.size(0)\n",
    "        slot = torch.LongTensor(self.slot_embed_idx).to(input_ids.device)  ##\n",
    "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
    "        J = slot_e.size(0)\n",
    "\n",
    "        all_point_outputs = torch.zeros(batch_size, J, max_len, self.vocab_size).to(\n",
    "            input_ids.device\n",
    "        )\n",
    "        \n",
    "        # Parallel Decoding\n",
    "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
    "        hidden = hidden.repeat_interleave(J, dim=1)\n",
    "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
    "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
    "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
    "        for k in range(max_len):\n",
    "            w = self.dropout(w)\n",
    "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
    "\n",
    "            # B,T,D * B,D,1 => B,T\n",
    "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
    "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e9)\n",
    "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
    "\n",
    "            if self.proj_layer:\n",
    "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
    "            else:\n",
    "                hidden_proj = hidden\n",
    "\n",
    "            # B,D * D,V => B,V\n",
    "            attn_v = torch.matmul(\n",
    "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
    "            )  # B,V\n",
    "            attn_vocab = F.softmax(attn_v, -1)\n",
    "\n",
    "            # B,1,T * B,T,D => B,1,D\n",
    "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
    "            p_gen = self.sigmoid(\n",
    "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
    "            )  # B,1\n",
    "            p_gen = p_gen.squeeze(-1)\n",
    "\n",
    "            p_context_ptr = torch.zeros_like(attn_vocab).to(input_ids.device)\n",
    "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
    "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
    "            _, w_idx = p_final.max(-1)\n",
    "\n",
    "            if teacher is not None:\n",
    "                w = self.embedding(teacher[:, :, k]).transpose(0, 1).reshape(batch_size * J, 1, -1)\n",
    "            else:\n",
    "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
    "            if k == 0:\n",
    "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
    "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
    "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56421870",
   "metadata": {
    "id": "56421870"
   },
   "source": [
    "## 모델 및 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f2936",
   "metadata": {
    "id": "d26f2936",
    "outputId": "f8d96419-d1ba-4bf3-a923-3eb77b3b031f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyumin/Development/bc_dst/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "slot_vocab = []\n",
    "for slot in slot_meta:\n",
    "    slot_vocab.append(\n",
    "        tokenizer.encode(slot.replace('-', ' '),\n",
    "                         add_special_tokens=False)\n",
    "    )\n",
    "    \n",
    "config = BertConfig.from_pretrained('dsksd/bert-ko-small-minimal')\n",
    "config.model_name_or_path = 'dsksd/bert-ko-small-minimal'\n",
    "config.n_gate = len(processor.gating2id)\n",
    "config.proj_dim = None\n",
    "model = TRADE(config, slot_vocab, slot_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6d5e2",
   "metadata": {
    "id": "cbb6d5e2"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=16, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b706c4e",
   "metadata": {
    "id": "2b706c4e"
   },
   "source": [
    "## Optimizer & Scheduler 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8593e4b",
   "metadata": {
    "id": "d8593e4b"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0.1, num_training_steps=t_total\n",
    ")\n",
    "teacher_forcing = 0.5\n",
    "model.to(device)\n",
    "\n",
    "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
    "    mask = target.ne(pad_idx)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    log_probs_flat = torch.log(logits_flat)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / (mask.sum().float())\n",
    "    return loss\n",
    "\n",
    "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
    "loss_fnc_2 = nn.CrossEntropyLoss()  # gating\n",
    "loss_fnc_pretrain = nn.CrossEntropyLoss()  # MLM pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb73492",
   "metadata": {
    "id": "0fb73492"
   },
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf8687",
   "metadata": {
    "collapsed": true,
    "id": "8aaf8687",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "85617036-0949-469a-ea4a-d8abc9fdcf12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/3] [0/2894] 10.695285\n",
      "[0/3] [100/2894] 10.653587\n",
      "[0/3] [200/2894] 10.636821\n",
      "[0/3] [300/2894] 10.710348\n",
      "[0/3] [400/2894] 10.655028\n",
      "[0/3] [500/2894] 10.701962\n",
      "[0/3] [600/2894] 10.700084\n",
      "[0/3] [700/2894] 10.652074\n",
      "[0/3] [800/2894] 10.669184\n",
      "[0/3] [900/2894] 10.635167\n",
      "[0/3] [1000/2894] 10.661515\n",
      "[0/3] [1100/2894] 10.724567\n",
      "[0/3] [1200/2894] 10.658038\n",
      "[0/3] [1300/2894] 10.662727\n",
      "[0/3] [1400/2894] 10.691187\n",
      "[0/3] [1500/2894] 10.628413\n",
      "[0/3] [1600/2894] 10.704060\n",
      "[0/3] [1700/2894] 10.612057\n",
      "[0/3] [1800/2894] 10.658079\n",
      "[0/3] [1900/2894] 10.662621\n",
      "[0/3] [2000/2894] 10.598007\n",
      "[0/3] [2100/2894] 10.616374\n",
      "[0/3] [2200/2894] 10.652863\n",
      "[0/3] [2300/2894] 10.659072\n",
      "[0/3] [2400/2894] 10.626670\n",
      "[0/3] [2500/2894] 10.615333\n",
      "[0/3] [2600/2894] 10.674631\n",
      "[0/3] [2700/2894] 10.666609\n",
      "[0/3] [2800/2894] 10.630229\n",
      "[1/3] [0/2894] 10.631713\n",
      "[1/3] [100/2894] 10.700149\n",
      "[1/3] [200/2894] 10.703929\n",
      "[1/3] [300/2894] 10.622448\n",
      "[1/3] [400/2894] 10.626153\n",
      "[1/3] [500/2894] 10.639113\n",
      "[1/3] [600/2894] 10.647092\n",
      "[1/3] [700/2894] 10.687211\n",
      "[1/3] [800/2894] 10.623909\n",
      "[1/3] [900/2894] 10.674276\n",
      "[1/3] [1000/2894] 10.611043\n",
      "[1/3] [1100/2894] 10.646062\n",
      "[1/3] [1200/2894] 10.616122\n",
      "[1/3] [1300/2894] 10.655216\n",
      "[1/3] [1400/2894] 10.683242\n",
      "[1/3] [1500/2894] 10.606111\n",
      "[1/3] [1600/2894] 10.638369\n",
      "[1/3] [1700/2894] 10.633267\n",
      "[1/3] [1800/2894] 10.638502\n",
      "[1/3] [1900/2894] 10.682846\n",
      "[1/3] [2000/2894] 10.683753\n",
      "[1/3] [2100/2894] 10.632410\n",
      "[1/3] [2200/2894] 10.668610\n",
      "[1/3] [2300/2894] 10.661909\n",
      "[1/3] [2400/2894] 10.646595\n",
      "[1/3] [2500/2894] 10.619462\n",
      "[1/3] [2600/2894] 10.623145\n",
      "[1/3] [2700/2894] 10.649871\n",
      "[1/3] [2800/2894] 10.632030\n",
      "[2/3] [0/2894] 10.626029\n",
      "[2/3] [100/2894] 10.682890\n",
      "[2/3] [200/2894] 10.725339\n",
      "[2/3] [300/2894] 10.661327\n",
      "[2/3] [400/2894] 10.699715\n",
      "[2/3] [500/2894] 10.656499\n",
      "[2/3] [600/2894] 10.675056\n",
      "[2/3] [700/2894] 10.611928\n",
      "[2/3] [800/2894] 10.663997\n",
      "[2/3] [900/2894] 10.667115\n",
      "[2/3] [1000/2894] 10.662878\n",
      "[2/3] [1100/2894] 10.660992\n",
      "[2/3] [1200/2894] 10.677027\n",
      "[2/3] [1300/2894] 10.660157\n",
      "[2/3] [1400/2894] 10.669881\n",
      "[2/3] [1500/2894] 10.636113\n",
      "[2/3] [1600/2894] 10.631918\n",
      "[2/3] [1700/2894] 10.664103\n",
      "[2/3] [1800/2894] 10.651160\n",
      "[2/3] [1900/2894] 10.635400\n",
      "[2/3] [2000/2894] 10.671768\n",
      "[2/3] [2100/2894] 10.639665\n",
      "[2/3] [2200/2894] 10.667610\n",
      "[2/3] [2300/2894] 10.656078\n",
      "[2/3] [2400/2894] 10.650519\n",
      "[2/3] [2500/2894] 10.652350\n",
      "[2/3] [2600/2894] 10.621589\n",
      "[2/3] [2700/2894] 10.629611\n",
      "[2/3] [2800/2894] 10.666641\n"
     ]
    }
   ],
   "source": [
    "MLM_PRE = True\n",
    "\n",
    "n_pretrain_epochs = 3\n",
    "\n",
    "def mlm_pretrain(loader, n_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "\n",
    "        logits, labels = model.forward_pretrain(input_ids, tokenizer)\n",
    "        loss = loss_fnc_pretrain(logits.view(-1, config.vocab_size), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(loader), loss.item()))\n",
    "\n",
    "if MLM_PRE:\n",
    "    for epoch in range(n_pretrain_epochs):\n",
    "        mlm_pretrain(train_loader, n_pretrain_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9d8dd",
   "metadata": {
    "id": "58f9d8dd"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23538cd2",
   "metadata": {
    "collapsed": true,
    "id": "23538cd2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9456c9d1-d30e-419e-ed85-86ddd5854911",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/2894] 10.479241\n",
      "[0/10] [100/2894] 1.787377\n",
      "[0/10] [200/2894] 1.833614\n",
      "[0/10] [300/2894] 0.978634\n",
      "[0/10] [400/2894] 0.868383\n",
      "[0/10] [500/2894] 0.769875\n",
      "[0/10] [600/2894] 0.694434\n",
      "[0/10] [700/2894] 0.709479\n",
      "[0/10] [800/2894] 0.786163\n",
      "[0/10] [900/2894] 0.388131\n",
      "[0/10] [1000/2894] 0.507657\n",
      "[0/10] [1100/2894] 0.517074\n",
      "[0/10] [1200/2894] 0.387976\n",
      "[0/10] [1300/2894] 0.369454\n",
      "[0/10] [1400/2894] 0.304115\n",
      "[0/10] [1500/2894] 0.390003\n",
      "[0/10] [1600/2894] 0.355167\n",
      "[0/10] [1700/2894] 0.350109\n",
      "[0/10] [1800/2894] 0.284057\n",
      "[0/10] [1900/2894] 0.342065\n",
      "[0/10] [2000/2894] 0.346315\n",
      "[0/10] [2100/2894] 0.297524\n",
      "[0/10] [2200/2894] 0.196061\n",
      "[0/10] [2300/2894] 0.283494\n",
      "[0/10] [2400/2894] 0.308459\n",
      "[0/10] [2500/2894] 0.191082\n",
      "[0/10] [2600/2894] 0.247932\n",
      "[0/10] [2700/2894] 0.379054\n",
      "[0/10] [2800/2894] 0.303518\n",
      "[0/10] [0/2894] 10.290485\n",
      "[0/10] [100/2894] 3.807715\n",
      "[0/10] [200/2894] 2.556649\n",
      "[0/10] [300/2894] 2.512274\n",
      "[0/10] [400/2894] 1.989473\n",
      "[0/10] [500/2894] 1.912938\n",
      "[0/10] [600/2894] 1.531750\n",
      "[0/10] [700/2894] 1.754463\n",
      "[0/10] [800/2894] 1.630143\n",
      "[0/10] [900/2894] 1.436543\n",
      "[0/10] [1000/2894] 1.641465\n",
      "[0/10] [1100/2894] 1.432701\n",
      "[0/10] [1200/2894] 1.201424\n",
      "[0/10] [1300/2894] 1.163717\n",
      "[0/10] [1400/2894] 1.046666\n",
      "[0/10] [1500/2894] 1.246151\n",
      "[0/10] [1600/2894] 1.269904\n",
      "[0/10] [1700/2894] 1.389812\n",
      "[0/10] [1800/2894] 1.000237\n",
      "[0/10] [1900/2894] 1.064126\n",
      "[0/10] [2000/2894] 1.038323\n",
      "[0/10] [2100/2894] 0.931134\n",
      "[0/10] [2200/2894] 1.180103\n",
      "[0/10] [2300/2894] 0.695853\n",
      "[0/10] [2400/2894] 0.837866\n",
      "[0/10] [2500/2894] 1.047203\n",
      "[0/10] [2600/2894] 1.004578\n",
      "[0/10] [2700/2894] 1.022385\n",
      "[0/10] [2800/2894] 0.814854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:09<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.001211876388608362, 'turn_slot_accuracy': 0.6366839471262775, 'turn_slot_f1': 0.26452292652714}\n",
      "joint_goal_accuracy: 0.001211876388608362\n",
      "turn_slot_accuracy: 0.6366839471262775\n",
      "turn_slot_f1: 0.26452292652714\n",
      "[1/10] [0/2894] 2.698599\n",
      "[1/10] [100/2894] 0.213983\n",
      "[1/10] [200/2894] 0.266393\n",
      "[1/10] [300/2894] 0.285038\n",
      "[1/10] [400/2894] 0.236070\n",
      "[1/10] [500/2894] 0.199817\n",
      "[1/10] [600/2894] 0.136715\n",
      "[1/10] [700/2894] 0.231997\n",
      "[1/10] [800/2894] 0.192494\n",
      "[1/10] [900/2894] 0.221587\n",
      "[1/10] [1000/2894] 0.227034\n",
      "[1/10] [1100/2894] 0.148866\n",
      "[1/10] [1200/2894] 0.134970\n",
      "[1/10] [1300/2894] 0.207710\n",
      "[1/10] [1400/2894] 0.199708\n",
      "[1/10] [1500/2894] 0.238266\n",
      "[1/10] [1600/2894] 0.125722\n",
      "[1/10] [1700/2894] 0.207651\n",
      "[1/10] [1800/2894] 0.111876\n",
      "[1/10] [1900/2894] 0.130565\n",
      "[1/10] [2000/2894] 0.204489\n",
      "[1/10] [2100/2894] 0.094385\n",
      "[1/10] [2200/2894] 0.191717\n",
      "[1/10] [2300/2894] 0.126738\n",
      "[1/10] [2400/2894] 0.113447\n",
      "[1/10] [2500/2894] 0.134092\n",
      "[1/10] [2600/2894] 0.172208\n",
      "[1/10] [2700/2894] 0.102315\n",
      "[1/10] [2800/2894] 0.155916\n",
      "[1/10] [0/2894] 2.578012\n",
      "[1/10] [100/2894] 0.883481\n",
      "[1/10] [200/2894] 0.941355\n",
      "[1/10] [300/2894] 0.744772\n",
      "[1/10] [400/2894] 0.828615\n",
      "[1/10] [500/2894] 0.826615\n",
      "[1/10] [600/2894] 0.890782\n",
      "[1/10] [700/2894] 0.967482\n",
      "[1/10] [800/2894] 0.809214\n",
      "[1/10] [900/2894] 0.693687\n",
      "[1/10] [1000/2894] 0.745883\n",
      "[1/10] [1100/2894] 0.807303\n",
      "[1/10] [1200/2894] 0.919360\n",
      "[1/10] [1300/2894] 0.631268\n",
      "[1/10] [1400/2894] 0.864993\n",
      "[1/10] [1500/2894] 0.768034\n",
      "[1/10] [1600/2894] 0.775684\n",
      "[1/10] [1700/2894] 0.738499\n",
      "[1/10] [1800/2894] 0.774894\n",
      "[1/10] [1900/2894] 0.652372\n",
      "[1/10] [2000/2894] 0.973642\n",
      "[1/10] [2100/2894] 0.635234\n",
      "[1/10] [2200/2894] 0.667830\n",
      "[1/10] [2300/2894] 0.889487\n",
      "[1/10] [2400/2894] 0.652195\n",
      "[1/10] [2500/2894] 0.893284\n",
      "[1/10] [2600/2894] 0.558812\n",
      "[1/10] [2700/2894] 0.706682\n",
      "[1/10] [2800/2894] 0.789631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.01979398101393658, 'turn_slot_accuracy': 0.8120469489889746, 'turn_slot_f1': 0.4722378969176701}\n",
      "joint_goal_accuracy: 0.01979398101393658\n",
      "turn_slot_accuracy: 0.8120469489889746\n",
      "turn_slot_f1: 0.4722378969176701\n",
      "[2/10] [0/2894] 1.636626\n",
      "[2/10] [100/2894] 0.105757\n",
      "[2/10] [200/2894] 0.065617\n",
      "[2/10] [300/2894] 0.117363\n",
      "[2/10] [400/2894] 0.163464\n",
      "[2/10] [500/2894] 0.058478\n",
      "[2/10] [600/2894] 0.081168\n",
      "[2/10] [700/2894] 0.073699\n",
      "[2/10] [800/2894] 0.100143\n",
      "[2/10] [900/2894] 0.097578\n",
      "[2/10] [1000/2894] 0.055120\n",
      "[2/10] [1100/2894] 0.124482\n",
      "[2/10] [1200/2894] 0.107333\n",
      "[2/10] [1300/2894] 0.128425\n",
      "[2/10] [1400/2894] 0.084379\n",
      "[2/10] [1500/2894] 0.067071\n",
      "[2/10] [1600/2894] 0.058993\n",
      "[2/10] [1700/2894] 0.046455\n",
      "[2/10] [1800/2894] 0.092474\n",
      "[2/10] [1900/2894] 0.083181\n",
      "[2/10] [2000/2894] 0.055736\n",
      "[2/10] [2100/2894] 0.069237\n",
      "[2/10] [2200/2894] 0.135437\n",
      "[2/10] [2300/2894] 0.118794\n",
      "[2/10] [2400/2894] 0.098667\n",
      "[2/10] [2500/2894] 0.086952\n",
      "[2/10] [2600/2894] 0.107385\n",
      "[2/10] [2700/2894] 0.087255\n",
      "[2/10] [2800/2894] 0.058802\n",
      "[2/10] [0/2894] 1.349305\n",
      "[2/10] [100/2894] 0.684125\n",
      "[2/10] [200/2894] 0.648754\n",
      "[2/10] [300/2894] 0.673301\n",
      "[2/10] [400/2894] 0.891340\n",
      "[2/10] [500/2894] 0.654733\n",
      "[2/10] [600/2894] 0.746208\n",
      "[2/10] [700/2894] 0.642893\n",
      "[2/10] [800/2894] 0.612745\n",
      "[2/10] [900/2894] 0.709113\n",
      "[2/10] [1000/2894] 0.755732\n",
      "[2/10] [1100/2894] 0.546828\n",
      "[2/10] [1200/2894] 0.611782\n",
      "[2/10] [1300/2894] 0.500536\n",
      "[2/10] [1400/2894] 0.711566\n",
      "[2/10] [1500/2894] 0.526780\n",
      "[2/10] [1600/2894] 0.645041\n",
      "[2/10] [1700/2894] 0.686699\n",
      "[2/10] [1800/2894] 0.722779\n",
      "[2/10] [1900/2894] 0.792461\n",
      "[2/10] [2000/2894] 0.790588\n",
      "[2/10] [2100/2894] 0.871485\n",
      "[2/10] [2200/2894] 0.640923\n",
      "[2/10] [2300/2894] 0.688833\n",
      "[2/10] [2400/2894] 0.545961\n",
      "[2/10] [2500/2894] 0.614169\n",
      "[2/10] [2600/2894] 0.613143\n",
      "[2/10] [2700/2894] 0.601874\n",
      "[2/10] [2800/2894] 0.486721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.06018986063421531, 'turn_slot_accuracy': 0.9033685675172273, 'turn_slot_f1': 0.6589605144883941}\n",
      "joint_goal_accuracy: 0.06018986063421531\n",
      "turn_slot_accuracy: 0.9033685675172273\n",
      "turn_slot_f1: 0.6589605144883941\n",
      "[3/10] [0/2894] 0.896906\n",
      "[3/10] [100/2894] 0.072288\n",
      "[3/10] [200/2894] 0.095614\n",
      "[3/10] [300/2894] 0.072446\n",
      "[3/10] [400/2894] 0.104356\n",
      "[3/10] [500/2894] 0.091745\n",
      "[3/10] [600/2894] 0.044922\n",
      "[3/10] [700/2894] 0.068284\n",
      "[3/10] [800/2894] 0.064532\n",
      "[3/10] [900/2894] 0.052464\n",
      "[3/10] [1000/2894] 0.062923\n",
      "[3/10] [1100/2894] 0.158064\n",
      "[3/10] [1200/2894] 0.121072\n",
      "[3/10] [1300/2894] 0.091380\n",
      "[3/10] [1400/2894] 0.051162\n",
      "[3/10] [1500/2894] 0.060938\n",
      "[3/10] [1600/2894] 0.090415\n",
      "[3/10] [1700/2894] 0.069550\n",
      "[3/10] [1800/2894] 0.056941\n",
      "[3/10] [1900/2894] 0.111795\n",
      "[3/10] [2000/2894] 0.053701\n",
      "[3/10] [2100/2894] 0.060008\n",
      "[3/10] [2200/2894] 0.081920\n",
      "[3/10] [2300/2894] 0.094612\n",
      "[3/10] [2400/2894] 0.037036\n",
      "[3/10] [2500/2894] 0.060814\n",
      "[3/10] [2600/2894] 0.081578\n",
      "[3/10] [2700/2894] 0.049486\n",
      "[3/10] [2800/2894] 0.064079\n",
      "[3/10] [0/2894] 0.954952\n",
      "[3/10] [100/2894] 0.639821\n",
      "[3/10] [200/2894] 0.633748\n",
      "[3/10] [300/2894] 0.428516\n",
      "[3/10] [400/2894] 0.551162\n",
      "[3/10] [500/2894] 0.903407\n",
      "[3/10] [600/2894] 0.793859\n",
      "[3/10] [700/2894] 0.522956\n",
      "[3/10] [800/2894] 0.722487\n",
      "[3/10] [900/2894] 0.625471\n",
      "[3/10] [1000/2894] 0.590044\n",
      "[3/10] [1100/2894] 0.542148\n",
      "[3/10] [1200/2894] 0.705040\n",
      "[3/10] [1300/2894] 0.615912\n",
      "[3/10] [1400/2894] 0.480685\n",
      "[3/10] [1500/2894] 0.649411\n",
      "[3/10] [1600/2894] 0.575953\n",
      "[3/10] [1700/2894] 0.568875\n",
      "[3/10] [1800/2894] 0.508229\n",
      "[3/10] [1900/2894] 0.621595\n",
      "[3/10] [2000/2894] 0.555142\n",
      "[3/10] [2100/2894] 0.477253\n",
      "[3/10] [2200/2894] 0.520413\n",
      "[3/10] [2300/2894] 0.664263\n",
      "[3/10] [2400/2894] 0.391132\n",
      "[3/10] [2500/2894] 0.483807\n",
      "[3/10] [2600/2894] 0.506037\n",
      "[3/10] [2700/2894] 0.643685\n",
      "[3/10] [2800/2894] 0.567404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.12643910321147242, 'turn_slot_accuracy': 0.9348324693103632, 'turn_slot_f1': 0.743428973393871}\n",
      "joint_goal_accuracy: 0.12643910321147242\n",
      "turn_slot_accuracy: 0.9348324693103632\n",
      "turn_slot_f1: 0.743428973393871\n",
      "[4/10] [0/2894] 0.375657\n",
      "[4/10] [100/2894] 0.048460\n",
      "[4/10] [200/2894] 0.100936\n",
      "[4/10] [300/2894] 0.112913\n",
      "[4/10] [400/2894] 0.058470\n",
      "[4/10] [500/2894] 0.076045\n",
      "[4/10] [600/2894] 0.056451\n",
      "[4/10] [700/2894] 0.024079\n",
      "[4/10] [800/2894] 0.032442\n",
      "[4/10] [900/2894] 0.054822\n",
      "[4/10] [1000/2894] 0.062688\n",
      "[4/10] [1100/2894] 0.098679\n",
      "[4/10] [1200/2894] 0.068616\n",
      "[4/10] [1300/2894] 0.016514\n",
      "[4/10] [1400/2894] 0.024989\n",
      "[4/10] [1500/2894] 0.076372\n",
      "[4/10] [1600/2894] 0.062778\n",
      "[4/10] [1700/2894] 0.060029\n",
      "[4/10] [1800/2894] 0.035235\n",
      "[4/10] [1900/2894] 0.072962\n",
      "[4/10] [2000/2894] 0.066529\n",
      "[4/10] [2100/2894] 0.076374\n",
      "[4/10] [2200/2894] 0.062498\n",
      "[4/10] [2300/2894] 0.073810\n",
      "[4/10] [2400/2894] 0.038563\n",
      "[4/10] [2500/2894] 0.048721\n",
      "[4/10] [2600/2894] 0.078993\n",
      "[4/10] [2700/2894] 0.040708\n",
      "[4/10] [2800/2894] 0.048131\n",
      "[4/10] [0/2894] 0.700684\n",
      "[4/10] [100/2894] 0.484845\n",
      "[4/10] [200/2894] 0.446423\n",
      "[4/10] [300/2894] 0.555048\n",
      "[4/10] [400/2894] 0.376008\n",
      "[4/10] [500/2894] 0.622070\n",
      "[4/10] [600/2894] 0.533501\n",
      "[4/10] [700/2894] 0.586872\n",
      "[4/10] [800/2894] 0.490120\n",
      "[4/10] [900/2894] 0.523058\n",
      "[4/10] [1000/2894] 0.464367\n",
      "[4/10] [1100/2894] 0.586287\n",
      "[4/10] [1200/2894] 0.567913\n",
      "[4/10] [1300/2894] 0.569258\n",
      "[4/10] [1400/2894] 0.384876\n",
      "[4/10] [1500/2894] 0.518551\n",
      "[4/10] [1600/2894] 0.782809\n",
      "[4/10] [1700/2894] 0.401337\n",
      "[4/10] [1800/2894] 0.463241\n",
      "[4/10] [1900/2894] 0.586658\n",
      "[4/10] [2000/2894] 0.432258\n",
      "[4/10] [2100/2894] 0.482113\n",
      "[4/10] [2200/2894] 0.547806\n",
      "[4/10] [2300/2894] 0.541928\n",
      "[4/10] [2400/2894] 0.563103\n",
      "[4/10] [2500/2894] 0.463686\n",
      "[4/10] [2600/2894] 0.697180\n",
      "[4/10] [2700/2894] 0.603072\n",
      "[4/10] [2800/2894] 0.514040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.30478691173500305, 'turn_slot_accuracy': 0.9656500370295695, 'turn_slot_f1': 0.8535739796875405}\n",
      "joint_goal_accuracy: 0.30478691173500305\n",
      "turn_slot_accuracy: 0.9656500370295695\n",
      "turn_slot_f1: 0.8535739796875405\n",
      "[5/10] [0/2894] 0.289729\n",
      "[5/10] [100/2894] 0.065920\n",
      "[5/10] [200/2894] 0.080398\n",
      "[5/10] [300/2894] 0.035892\n",
      "[5/10] [400/2894] 0.033895\n",
      "[5/10] [500/2894] 0.046071\n",
      "[5/10] [600/2894] 0.021943\n",
      "[5/10] [700/2894] 0.055971\n",
      "[5/10] [800/2894] 0.026725\n",
      "[5/10] [900/2894] 0.025048\n",
      "[5/10] [1000/2894] 0.045479\n",
      "[5/10] [1100/2894] 0.047929\n",
      "[5/10] [1200/2894] 0.022669\n",
      "[5/10] [1300/2894] 0.082602\n",
      "[5/10] [1400/2894] 0.042442\n",
      "[5/10] [1500/2894] 0.076513\n",
      "[5/10] [1600/2894] 0.047413\n",
      "[5/10] [1700/2894] 0.036124\n",
      "[5/10] [1800/2894] 0.041714\n",
      "[5/10] [1900/2894] 0.024631\n",
      "[5/10] [2000/2894] 0.039686\n",
      "[5/10] [2100/2894] 0.039790\n",
      "[5/10] [2200/2894] 0.037568\n",
      "[5/10] [2300/2894] 0.035502\n",
      "[5/10] [2400/2894] 0.072325\n",
      "[5/10] [2500/2894] 0.025523\n",
      "[5/10] [2600/2894] 0.034588\n",
      "[5/10] [2700/2894] 0.045888\n",
      "[5/10] [2800/2894] 0.030930\n",
      "[5/10] [0/2894] 0.774105\n",
      "[5/10] [100/2894] 0.620906\n",
      "[5/10] [200/2894] 0.579740\n",
      "[5/10] [300/2894] 0.580315\n",
      "[5/10] [400/2894] 0.476849\n",
      "[5/10] [500/2894] 0.489938\n",
      "[5/10] [600/2894] 0.617216\n",
      "[5/10] [700/2894] 0.562254\n",
      "[5/10] [800/2894] 0.476278\n",
      "[5/10] [900/2894] 0.376025\n",
      "[5/10] [1000/2894] 0.466595\n",
      "[5/10] [1100/2894] 0.505516\n",
      "[5/10] [1200/2894] 0.552512\n",
      "[5/10] [1300/2894] 0.631606\n",
      "[5/10] [1400/2894] 0.497099\n",
      "[5/10] [1500/2894] 0.462190\n",
      "[5/10] [1600/2894] 0.538976\n",
      "[5/10] [1700/2894] 0.381597\n",
      "[5/10] [1800/2894] 0.552667\n",
      "[5/10] [1900/2894] 0.618836\n",
      "[5/10] [2000/2894] 0.506341\n",
      "[5/10] [2100/2894] 0.488736\n",
      "[5/10] [2200/2894] 0.526985\n",
      "[5/10] [2300/2894] 0.454575\n",
      "[5/10] [2400/2894] 0.508937\n",
      "[5/10] [2500/2894] 0.467481\n",
      "[5/10] [2600/2894] 0.555160\n",
      "[5/10] [2700/2894] 0.558487\n",
      "[5/10] [2800/2894] 0.445448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.3983033730559483, 'turn_slot_accuracy': 0.9731816243632128, 'turn_slot_f1': 0.8865022788453295}\n",
      "joint_goal_accuracy: 0.3983033730559483\n",
      "turn_slot_accuracy: 0.9731816243632128\n",
      "turn_slot_f1: 0.8865022788453295\n",
      "[6/10] [0/2894] 0.216665\n",
      "[6/10] [100/2894] 0.043218\n",
      "[6/10] [200/2894] 0.057242\n",
      "[6/10] [300/2894] 0.056206\n",
      "[6/10] [400/2894] 0.044075\n",
      "[6/10] [500/2894] 0.039929\n",
      "[6/10] [600/2894] 0.055174\n",
      "[6/10] [700/2894] 0.039498\n",
      "[6/10] [800/2894] 0.030613\n",
      "[6/10] [900/2894] 0.032366\n",
      "[6/10] [1000/2894] 0.119888\n",
      "[6/10] [1100/2894] 0.024544\n",
      "[6/10] [1200/2894] 0.037841\n",
      "[6/10] [1300/2894] 0.032522\n",
      "[6/10] [1400/2894] 0.016029\n",
      "[6/10] [1500/2894] 0.019364\n",
      "[6/10] [1600/2894] 0.034022\n",
      "[6/10] [1700/2894] 0.047358\n",
      "[6/10] [1800/2894] 0.039442\n",
      "[6/10] [1900/2894] 0.037178\n",
      "[6/10] [2000/2894] 0.030836\n",
      "[6/10] [2100/2894] 0.055623\n",
      "[6/10] [2200/2894] 0.030512\n",
      "[6/10] [2300/2894] 0.019074\n",
      "[6/10] [2400/2894] 0.032463\n",
      "[6/10] [2500/2894] 0.025576\n",
      "[6/10] [2600/2894] 0.056333\n",
      "[6/10] [2700/2894] 0.037195\n",
      "[6/10] [2800/2894] 0.035544\n",
      "[6/10] [0/2894] 0.599620\n",
      "[6/10] [100/2894] 0.426242\n",
      "[6/10] [200/2894] 0.481542\n",
      "[6/10] [300/2894] 0.534859\n",
      "[6/10] [400/2894] 0.402545\n",
      "[6/10] [500/2894] 0.503560\n",
      "[6/10] [600/2894] 0.392838\n",
      "[6/10] [700/2894] 0.546052\n",
      "[6/10] [800/2894] 0.419252\n",
      "[6/10] [900/2894] 0.568001\n",
      "[6/10] [1000/2894] 0.426298\n",
      "[6/10] [1100/2894] 0.444995\n",
      "[6/10] [1200/2894] 0.509952\n",
      "[6/10] [1300/2894] 0.518683\n",
      "[6/10] [1400/2894] 0.447471\n",
      "[6/10] [1500/2894] 0.362948\n",
      "[6/10] [1600/2894] 0.434126\n",
      "[6/10] [1700/2894] 0.584123\n",
      "[6/10] [1800/2894] 0.392963\n",
      "[6/10] [1900/2894] 0.520228\n",
      "[6/10] [2000/2894] 0.478046\n",
      "[6/10] [2100/2894] 0.422811\n",
      "[6/10] [2200/2894] 0.488687\n",
      "[6/10] [2300/2894] 0.503192\n",
      "[6/10] [2400/2894] 0.355366\n",
      "[6/10] [2500/2894] 0.450481\n",
      "[6/10] [2600/2894] 0.389834\n",
      "[6/10] [2700/2894] 0.626515\n",
      "[6/10] [2800/2894] 0.686812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.5406988487174308, 'turn_slot_accuracy': 0.9837967638412062, 'turn_slot_f1': 0.9209380437252567}\n",
      "joint_goal_accuracy: 0.5406988487174308\n",
      "turn_slot_accuracy: 0.9837967638412062\n",
      "turn_slot_f1: 0.9209380437252567\n",
      "[7/10] [0/2894] 0.058712\n",
      "[7/10] [100/2894] 0.034744\n",
      "[7/10] [200/2894] 0.064716\n",
      "[7/10] [300/2894] 0.046700\n",
      "[7/10] [400/2894] 0.023303\n",
      "[7/10] [500/2894] 0.025220\n",
      "[7/10] [600/2894] 0.029212\n",
      "[7/10] [700/2894] 0.017783\n",
      "[7/10] [800/2894] 0.052295\n",
      "[7/10] [900/2894] 0.016869\n",
      "[7/10] [1000/2894] 0.010020\n",
      "[7/10] [1100/2894] 0.082434\n",
      "[7/10] [1200/2894] 0.029836\n",
      "[7/10] [1300/2894] 0.024994\n",
      "[7/10] [1400/2894] 0.027071\n",
      "[7/10] [1500/2894] 0.033265\n",
      "[7/10] [1600/2894] 0.027167\n",
      "[7/10] [1700/2894] 0.018439\n",
      "[7/10] [1800/2894] 0.050906\n",
      "[7/10] [1900/2894] 0.085401\n",
      "[7/10] [2000/2894] 0.061284\n",
      "[7/10] [2100/2894] 0.055016\n",
      "[7/10] [2200/2894] 0.043745\n",
      "[7/10] [2300/2894] 0.046031\n",
      "[7/10] [2400/2894] 0.058996\n",
      "[7/10] [2500/2894] 0.014377\n",
      "[7/10] [2600/2894] 0.037652\n",
      "[7/10] [2700/2894] 0.022506\n",
      "[7/10] [2800/2894] 0.010236\n",
      "[7/10] [0/2894] 0.385933\n",
      "[7/10] [100/2894] 0.471829\n",
      "[7/10] [200/2894] 0.539484\n",
      "[7/10] [300/2894] 0.401755\n",
      "[7/10] [400/2894] 0.433960\n",
      "[7/10] [500/2894] 0.506947\n",
      "[7/10] [600/2894] 0.366353\n",
      "[7/10] [700/2894] 0.363523\n",
      "[7/10] [800/2894] 0.662734\n",
      "[7/10] [900/2894] 0.758151\n",
      "[7/10] [1000/2894] 0.613593\n",
      "[7/10] [1100/2894] 0.485419\n",
      "[7/10] [1200/2894] 0.337675\n",
      "[7/10] [1300/2894] 0.370533\n",
      "[7/10] [1400/2894] 0.550339\n",
      "[7/10] [1500/2894] 0.489599\n",
      "[7/10] [1600/2894] 0.416233\n",
      "[7/10] [1700/2894] 0.443643\n",
      "[7/10] [1800/2894] 0.451862\n",
      "[7/10] [1900/2894] 0.378233\n",
      "[7/10] [2000/2894] 0.442508\n",
      "[7/10] [2100/2894] 0.469342\n",
      "[7/10] [2200/2894] 0.424131\n",
      "[7/10] [2300/2894] 0.458116\n",
      "[7/10] [2400/2894] 0.406077\n",
      "[7/10] [2500/2894] 0.415132\n",
      "[7/10] [2600/2894] 0.368811\n",
      "[7/10] [2700/2894] 0.476778\n",
      "[7/10] [2800/2894] 0.342401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.5901838012522723, 'turn_slot_accuracy': 0.9861217711349072, 'turn_slot_f1': 0.932642909758438}\n",
      "joint_goal_accuracy: 0.5901838012522723\n",
      "turn_slot_accuracy: 0.9861217711349072\n",
      "turn_slot_f1: 0.932642909758438\n",
      "[8/10] [0/2894] 0.039473\n",
      "[8/10] [100/2894] 0.116277\n",
      "[8/10] [200/2894] 0.030894\n",
      "[8/10] [300/2894] 0.017684\n",
      "[8/10] [400/2894] 0.020432\n",
      "[8/10] [500/2894] 0.063957\n",
      "[8/10] [600/2894] 0.021164\n",
      "[8/10] [700/2894] 0.036350\n",
      "[8/10] [800/2894] 0.054530\n",
      "[8/10] [900/2894] 0.041291\n",
      "[8/10] [1000/2894] 0.025891\n",
      "[8/10] [1100/2894] 0.048146\n",
      "[8/10] [1200/2894] 0.026129\n",
      "[8/10] [1300/2894] 0.028268\n",
      "[8/10] [1400/2894] 0.025651\n",
      "[8/10] [1500/2894] 0.031688\n",
      "[8/10] [1600/2894] 0.016329\n",
      "[8/10] [1700/2894] 0.023373\n",
      "[8/10] [1800/2894] 0.023661\n",
      "[8/10] [1900/2894] 0.029619\n",
      "[8/10] [2000/2894] 0.034652\n",
      "[8/10] [2100/2894] 0.043870\n",
      "[8/10] [2200/2894] 0.018598\n",
      "[8/10] [2300/2894] 0.036970\n",
      "[8/10] [2400/2894] 0.022005\n",
      "[8/10] [2500/2894] 0.034227\n",
      "[8/10] [2600/2894] 0.021552\n",
      "[8/10] [2700/2894] 0.012820\n",
      "[8/10] [2800/2894] 0.022684\n",
      "[8/10] [0/2894] 0.238787\n",
      "[8/10] [100/2894] 0.458755\n",
      "[8/10] [200/2894] 0.511465\n",
      "[8/10] [300/2894] 0.606669\n",
      "[8/10] [400/2894] 0.443756\n",
      "[8/10] [500/2894] 0.407656\n",
      "[8/10] [600/2894] 0.610494\n",
      "[8/10] [700/2894] 0.378296\n",
      "[8/10] [800/2894] 0.464518\n",
      "[8/10] [900/2894] 0.384778\n",
      "[8/10] [1000/2894] 0.405003\n",
      "[8/10] [1100/2894] 0.389699\n",
      "[8/10] [1200/2894] 0.521936\n",
      "[8/10] [1300/2894] 0.431702\n",
      "[8/10] [1400/2894] 0.409698\n",
      "[8/10] [1500/2894] 0.428868\n",
      "[8/10] [1600/2894] 0.481784\n",
      "[8/10] [1700/2894] 0.556020\n",
      "[8/10] [1800/2894] 0.559428\n",
      "[8/10] [1900/2894] 0.419266\n",
      "[8/10] [2000/2894] 0.547934\n",
      "[8/10] [2100/2894] 0.448969\n",
      "[8/10] [2200/2894] 0.440844\n",
      "[8/10] [2300/2894] 0.493057\n",
      "[8/10] [2400/2894] 0.364085\n",
      "[8/10] [2500/2894] 0.404826\n",
      "[8/10] [2600/2894] 0.511066\n",
      "[8/10] [2700/2894] 0.353144\n",
      "[8/10] [2800/2894] 0.491457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.6154312260149465, 'turn_slot_accuracy': 0.9874054624206189, 'turn_slot_f1': 0.9388318799464411}\n",
      "joint_goal_accuracy: 0.6154312260149465\n",
      "turn_slot_accuracy: 0.9874054624206189\n",
      "turn_slot_f1: 0.9388318799464411\n",
      "[9/10] [0/2894] 0.042903\n",
      "[9/10] [100/2894] 0.025247\n",
      "[9/10] [200/2894] 0.039650\n",
      "[9/10] [300/2894] 0.034933\n",
      "[9/10] [400/2894] 0.043661\n",
      "[9/10] [500/2894] 0.012590\n",
      "[9/10] [600/2894] 0.023037\n",
      "[9/10] [700/2894] 0.019565\n",
      "[9/10] [800/2894] 0.039427\n",
      "[9/10] [900/2894] 0.014163\n",
      "[9/10] [1000/2894] 0.018687\n",
      "[9/10] [1100/2894] 0.026516\n",
      "[9/10] [1200/2894] 0.043638\n",
      "[9/10] [1300/2894] 0.034804\n",
      "[9/10] [1400/2894] 0.026990\n",
      "[9/10] [1500/2894] 0.029972\n",
      "[9/10] [1600/2894] 0.020106\n",
      "[9/10] [1700/2894] 0.019763\n",
      "[9/10] [1800/2894] 0.031618\n",
      "[9/10] [1900/2894] 0.020294\n",
      "[9/10] [2000/2894] 0.009980\n",
      "[9/10] [2100/2894] 0.073129\n",
      "[9/10] [2200/2894] 0.023514\n",
      "[9/10] [2300/2894] 0.013358\n",
      "[9/10] [2400/2894] 0.013117\n",
      "[9/10] [2500/2894] 0.026427\n",
      "[9/10] [2600/2894] 0.026022\n",
      "[9/10] [2700/2894] 0.021711\n",
      "[9/10] [2800/2894] 0.027196\n",
      "[9/10] [0/2894] 0.351226\n",
      "[9/10] [100/2894] 0.459361\n",
      "[9/10] [200/2894] 0.474037\n",
      "[9/10] [300/2894] 0.375618\n",
      "[9/10] [400/2894] 0.347561\n",
      "[9/10] [500/2894] 0.497267\n",
      "[9/10] [600/2894] 0.428543\n",
      "[9/10] [700/2894] 0.443435\n",
      "[9/10] [800/2894] 0.495795\n",
      "[9/10] [900/2894] 0.463731\n",
      "[9/10] [1000/2894] 0.394827\n",
      "[9/10] [1100/2894] 0.351387\n",
      "[9/10] [1200/2894] 0.438135\n",
      "[9/10] [1300/2894] 0.474104\n",
      "[9/10] [1400/2894] 0.420947\n",
      "[9/10] [1500/2894] 0.449991\n",
      "[9/10] [1600/2894] 0.480873\n",
      "[9/10] [1700/2894] 0.406990\n",
      "[9/10] [1800/2894] 0.411612\n",
      "[9/10] [1900/2894] 0.464625\n",
      "[9/10] [2000/2894] 0.500443\n",
      "[9/10] [2100/2894] 0.601577\n",
      "[9/10] [2200/2894] 0.408554\n",
      "[9/10] [2300/2894] 0.561632\n",
      "[9/10] [2400/2894] 0.510765\n",
      "[9/10] [2500/2894] 0.406749\n",
      "[9/10] [2600/2894] 0.467440\n",
      "[9/10] [2700/2894] 0.320806\n",
      "[9/10] [2800/2894] 0.505600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 619/619 [01:08<00:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.6241163401333064, 'turn_slot_accuracy': 0.9877420947507864, 'turn_slot_f1': 0.940597239821952}\n",
      "joint_goal_accuracy: 0.6241163401333064\n",
      "turn_slot_accuracy: 0.9877420947507864\n",
      "turn_slot_f1: 0.940597239821952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MLM_DURING = True\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "        if teacher_forcing > 0.0 and random.random() < teacher_forcing:\n",
    "            tf = target_ids\n",
    "        else:\n",
    "            tf = None\n",
    "\n",
    "        all_point_outputs, all_gate_outputs = model(input_ids, segment_ids, input_masks, target_ids.size(-1))  # gt - length (generation)\n",
    "        loss_1 = loss_fnc_1(all_point_outputs.contiguous(), target_ids.contiguous().view(-1))\n",
    "        loss_2 = loss_fnc_2(all_gate_outputs.contiguous().view(-1, 5), gating_ids.contiguous().view(-1))\n",
    "        loss = loss_1 + loss_2\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "            \n",
    "    if MLM_DURING:\n",
    "        mlm_pretrain(train_loader, n_epochs)\n",
    "                \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48021e08",
   "metadata": {
    "id": "48021e08"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cb1fb",
   "metadata": {
    "id": "b02cb1fb",
    "outputId": "1a99a246-b8f1-4365-b51b-774dff71091c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 36946.73it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=False, dialogue_level=False\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef9937",
   "metadata": {
    "id": "b2ef9937",
    "outputId": "6a6da7b0-2754-4b23-cf43-f5906932e0fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 944/944 [01:45<00:00,  8.92it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7163b",
   "metadata": {
    "id": "03a7163b"
   },
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efcf98",
   "metadata": {
    "id": "62efcf98"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TAPT_solution.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
