{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "bert",
      "language": "python",
      "name": "bert"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "TAPT-Pretraining.ipynbÏùò ÏÇ¨Î≥∏",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnA3xhhu0byu"
      },
      "source": [
        "# TAPT-Pretraining \n",
        "\n",
        "### Ïô∏Î∂Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÌôúÏö©ÌïòÏó¨ ÎÇòÎßåÏùò conversationBERTÎ•º ÎßåÎì§Ïñ¥Î≥¥ÏÑ∏Ïöî ! ü§ó\n",
        "\n",
        "Pretrained BERT + Ï∂îÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º ÌôúÏö©Ìïú Pretraining \n",
        "\n",
        "* Î≥∏ ÏΩîÎìúÎäî HuggingfaceÏôÄ BERT Ï†ÄÏûê ÏΩîÎìúÎ•º Í∏∞Î∞òÏúºÎ°ú ÏûëÏÑ±ÎêòÏóàÏäµÎãàÎã§"
      ],
      "id": "vnA3xhhu0byu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZxF7Si0by1"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from filelock import FileLock\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertConfig, BertForPreTraining\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers.utils import logging\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback # transformers 4.5.1ÏóêÏÑú Í∞ÄÎä•\n",
        "\n",
        "logger = logging.get_logger(__name__)"
      ],
      "id": "bgZxF7Si0by1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNBd04vC0by3"
      },
      "source": [
        "# set seed \n",
        "# reference : https://hoya012.github.io/blog/reproducible_pytorch/\n",
        "def set_seed(random_seed):\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)  # for multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    \n",
        "set_seed(42) "
      ],
      "id": "oNBd04vC0by3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XOIQctQ0by4"
      },
      "source": [
        "### ‚úÖ ÏÇ¨Ïö©Ìï¥Î≥ºÎßåÌïú Îç∞Ïù¥ÌÑ∞ÏÖãÎì§ \n",
        "1. Aihub  \n",
        "https://aihub.or.kr/aidata/85  \n",
        "\n",
        "2. ÌïúÍµ≠Ïñ¥ ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ Îç∞Ïù¥ÌÑ∞ÏÖã Î™©Î°ù   \n",
        "https://littlefoxdiary.tistory.com/42"
      ],
      "id": "4XOIQctQ0by4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFBfprmI0by4"
      },
      "source": [
        "#Ïó¨Îü¨Î∂ÑÏù¥ ÏÇ¨Ïö©ÌïòÏã§ Ï†ÅÏ†àÌïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î∂àÎü¨Ïò§ÏÖîÏïºÌï©ÎãàÎã§ !\n",
        "\n",
        "train_data_file = \"/opt/ml/input/data/train_dataset/train_dials.json\"\n",
        "dev_data_file = \"/opt/ml/input/data/eval_dataset/eval_dials.json\""
      ],
      "id": "YFBfprmI0by4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfZyJEuB0by4"
      },
      "source": [
        "## Pretraining Dataset\n",
        "\n",
        "PretrainingÏùÑ ÏúÑÌïú DatasetÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n",
        "\n",
        "1. Î≥∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Document Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÏùòÎêòÏñ¥ ÏûàÏäµÎãàÎã§. (Í∞Å Î¨∏Ïû• ÏÇ¨Ïù¥Îäî \\nÏúºÎ°ú, document ÏÇ¨Ïù¥Îäî \\n\\nÏúºÎ°ú Íµ¨Î∂ÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§) Ìï¥Îãπ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§Î•º Íµ¨Ìïú Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÎßûÏ∂∞ Î≥ÄÍ≤ΩÌï¥Ï£ºÏÑ∏Ïöî. \n",
        "    üëâüèª ÏòàÏãú : `data = ['sentence1 \\n', 'sentence2 \\n', 'setence3 \\n' ,'\\n' , 'sentence4 \\n', 'sentence5 \\n', 'sentence6 \\n', '\\n' ....]`\n",
        "\n",
        "\n",
        "2. ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Next Sentence PredictionÏùÑ ÏúÑÌïú Ï†ÑÏ≤òÎ¶¨ Í≥ºÏ†ïÎèÑ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§. MLMÎßåÏùÑ Ïù¥Ïö©ÌïòÏó¨ pretrainingÏùÑ ÏßÑÌñâÌï† Í≤ΩÏö∞ Ìï¥Îãπ Î∂ÄÎ∂ÑÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÏÇ¨Ïö©ÌïòÏãúÎ©¥ Îê©ÎãàÎã§.\n"
      ],
      "id": "BfZyJEuB0by4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW2vu_6s0by5"
      },
      "source": [
        "class TextDatasetForNextSentencePrediction(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        file_path,\n",
        "        block_size,\n",
        "        overwrite_cache=False,\n",
        "        short_seq_probability=0.1,\n",
        "        nsp_probability=0.5,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
        "        self.short_seq_probability = short_seq_probability\n",
        "        self.nsp_probability = nsp_probability\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory,\n",
        "            \"cached_nsp_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "\n",
        "        # Input file format:\n",
        "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "        # sentence boundaries for the \"next sentence prediction\" task).\n",
        "        # (2) Blank lines between documents. Document boundaries are needed so\n",
        "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "        #\n",
        "        # Example:\n",
        "        # I am very happy.\n",
        "        # Here is the second sentence.\n",
        "        #\n",
        "        # A new document.\n",
        "        \n",
        "        \n",
        "        # ‚úÖ Ï∫êÏãú ÌòïÌÉúÎ°ú ÌååÏùºÏùÑ Ï†ÄÏû•Ìï©ÎãàÎã§ \n",
        "        with FileLock(lock_path):\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\",\n",
        "                    time.time() - start,\n",
        "                )\n",
        "            else:\n",
        "                print(f\"Creating features from dataset file at {directory}\")\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                # Make dataset\n",
        "                self.documents = [[]]\n",
        "\n",
        "                # ‚úÖ Í∏∞Ï°¥ ÏΩîÎìúÏóî progress barÍ∞Ä ÏóÜÏñ¥ÏÑú Ï∂îÍ∞ÄÌïòÏòÄÏäµÎãàÎã§ : Í≥µÏãùÏΩîÎìúÎäî TQDMÏù¥ ÏóÜÏùå -> pbarÎ°ú Í±∏Ïñ¥Ï£ºÏûê\n",
        "                cnt = 0\n",
        "                count_data = len(open(file_path, \"r\", errors=\"ignore\").readlines())\n",
        "\n",
        "                pbar = tqdm(total=count_data)\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    while True:  \n",
        "                        line = f.readline()\n",
        "                        if not line:\n",
        "                            break\n",
        "                        line = line.strip()\n",
        "                        if not line and len(self.documents[-1]) != 0:\n",
        "                            self.documents.append([])\n",
        "                        tokens = tokenizer.tokenize(line)\n",
        "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                        if tokens:\n",
        "                            self.documents[-1].append(tokens)\n",
        "                        pbar.update(1)\n",
        "                pbar.close()\n",
        "\n",
        "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
        "                self.examples = []\n",
        "\n",
        "                for doc_index, document in enumerate(tqdm(self.documents)):\n",
        "                    self.create_examples_from_document(document, doc_index)  \n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\",\n",
        "                    cached_features_file,\n",
        "                    time.time() - start,\n",
        "                )\n",
        "\n",
        "    def create_examples_from_document(self, document, doc_index):\n",
        "        \"\"\"Creates examples for a single document.\"\"\"\n",
        "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(\n",
        "            pair=True\n",
        "        )\n",
        "\n",
        "        # We *usually* want to fill up the entire sequence since we are padding\n",
        "        # to `block_size` anyways, so short sequences are generally wasted\n",
        "        # computation. However, we *sometimes*\n",
        "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
        "        # The `target_seq_length` is just a rough target however, whereas\n",
        "        # `block_size` is a hard limit.\n",
        "\n",
        "        target_seq_length = max_num_tokens\n",
        "        if random.random() < self.short_seq_probability:\n",
        "            target_seq_length = random.randint(2, max_num_tokens)\n",
        "\n",
        "        current_chunk = []  # a buffer stored current working segments\n",
        "        current_length = 0\n",
        "        i = 0\n",
        "        \n",
        "        # ‚úÖ NSPÎ•º ÏúÑÌïú Preprocessing : NSPÍ∞Ä ÌïÑÏöîÏóÜÎã§Î©¥, Ïù¥ Î∂ÄÎ∂ÑÏùÑ Ï†úÏô∏Ìï¥Ï£ºÏÑ∏Ïöî \n",
        "        while i < len(document):\n",
        "            segment = document[i]\n",
        "            current_chunk.append(segment)\n",
        "            current_length += len(segment)\n",
        "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "                if current_chunk:\n",
        "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "                    # (first) sentence.\n",
        "                    a_end = 1\n",
        "                    if len(current_chunk) >= 2:\n",
        "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
        "                    tokens_a = []\n",
        "                    for j in range(a_end):\n",
        "                        tokens_a.extend(current_chunk[j])\n",
        "                    tokens_b = []\n",
        "                    if (\n",
        "                        len(current_chunk) == 1\n",
        "                        or random.random() < self.nsp_probability\n",
        "                    ):\n",
        "                        is_random_next = True\n",
        "                        target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "                        # This should rarely go for more than one iteration for large\n",
        "                        # corpora. However, just to be careful, we try to make sure that\n",
        "                        # the random document is not the same as the document\n",
        "                        # we're processing.\n",
        "                        for _ in range(10):\n",
        "                            random_document_index = random.randint(\n",
        "                                0, len(self.documents) - 1\n",
        "                            )\n",
        "                            if random_document_index != doc_index:\n",
        "                                break\n",
        "                        random_document = self.documents[random_document_index]\n",
        "                        random_start = random.randint(0, len(random_document) - 1)\n",
        "                        for j in range(random_start, len(random_document)):\n",
        "                            tokens_b.extend(random_document[j])\n",
        "                            if len(tokens_b) >= target_b_length:\n",
        "                                break\n",
        "                        # We didn't actually use these segments so we \"put them back\" so\n",
        "                        # they don't go to waste.\n",
        "                        num_unused_segments = len(current_chunk) - a_end\n",
        "                        i -= num_unused_segments\n",
        "                    else:\n",
        "                        is_random_next = False\n",
        "                        for j in range(a_end, len(current_chunk)):\n",
        "                            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
        "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "                        while True:\n",
        "                            total_length = len(tokens_a) + len(tokens_b)\n",
        "                            if total_length <= max_num_tokens:\n",
        "                                break\n",
        "                            trunc_tokens = (\n",
        "                                tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "                            )\n",
        "                            assert len(trunc_tokens) >= 1\n",
        "                            # We want to sometimes truncate from the front and sometimes from the\n",
        "                            # back to add more randomness and avoid biases.\n",
        "                            if random.random() < 0.5:\n",
        "                                del trunc_tokens[0]\n",
        "                            else:\n",
        "                                trunc_tokens.pop()\n",
        "\n",
        "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
        "\n",
        "                    assert len(tokens_a) >= 1\n",
        "                    assert len(tokens_b) >= 1\n",
        "\n",
        "                    # add special tokens\n",
        "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(\n",
        "                        tokens_a, tokens_b\n",
        "                    )\n",
        "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
        "                    token_type_ids = (\n",
        "                        self.tokenizer.create_token_type_ids_from_sequences(\n",
        "                            tokens_a, tokens_b\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "                    # ‚úÖ Îç∞Ïù¥ÌÑ∞Í∞Ä Ï†ÄÏû•ÎêòÎäî ÌòïÌÉú     \n",
        "                    example = {\n",
        "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "                        \"token_type_ids\": torch.tensor(\n",
        "                            token_type_ids, dtype=torch.long\n",
        "                        ),\n",
        "                        \"next_sentence_label\": torch.tensor(\n",
        "                            1 if is_random_next else 0, dtype=torch.long\n",
        "                        ),\n",
        "                    }\n",
        "\n",
        "                    # ‚úÖ Ï£ºÏùò : Ïù¥Î†áÍ≤å append ÌïòÎäî Î∞©ÏãùÏùÄ ÎåÄÏö©Îüâ corpusÏóêÏÑú Î©îÎ™®Î¶¨ Ïù¥ÏäàÎ•º Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§ ! \n",
        "                    self.examples.append(example)\n",
        "\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ],
      "id": "yW2vu_6s0by5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jz5ZDfX0by_"
      },
      "source": [
        "## Dataset"
      ],
      "id": "1jz5ZDfX0by_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "DSQsdzNw0by_",
        "outputId": "b9bd9602-a578-44f2-8818-98ab698cce10"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dsksd/bert-ko-small-minimal')\n",
        "\n",
        "# for dataset\n",
        "train_dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_data_file,\n",
        "    block_size=256,\n",
        "    overwrite_cache=False,\n",
        "    short_seq_probability=0.1,\n",
        "    nsp_probability=0.5,\n",
        ")\n",
        "\n",
        "dev_dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=dev_data_file,\n",
        "    block_size=256,\n",
        "    overwrite_cache=False,\n",
        "    short_seq_probability=0.1,\n",
        "    nsp_probability=0.5,\n",
        ")"
      ],
      "id": "DSQsdzNw0by_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Input file path /opt/ml/input/data/train_dataset/train_dials.json not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ad86cd989729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# for dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_dataset = TextDatasetForNextSentencePrediction(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bc3c6984c0ce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, short_seq_probability, nsp_probability)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnsp_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     ):\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Input file path {file_path} not found\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_special_tokens_to_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Input file path /opt/ml/input/data/train_dataset/train_dials.json not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCD4T2G10bzB",
        "outputId": "8c628a1e-f029-451d-e8a2-93cbded52650"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(dev_dataset))"
      ],
      "id": "sCD4T2G10bzB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46294\n",
            "4951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4MR4Pm90bzB"
      },
      "source": [
        "# Pretraining Model"
      ],
      "id": "p4MR4Pm90bzB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JiZo1Y90bzC"
      },
      "source": [
        "## Î™®Îç∏ Î∞è Îç∞Ïù¥ÌÑ∞ Collator Ï†ïÏùò"
      ],
      "id": "_JiZo1Y90bzC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIwp1qRD0bzC",
        "outputId": "be1d1f1c-4f86-4419-ebf6-c6d1f3d7c743"
      },
      "source": [
        "config = BertConfig.from_pretrained('dsksd/bert-ko-small-minimal')\n",
        "config.model_name_or_path = 'dsksd/bert-ko-small-minimal'\n",
        "config.n_gate = len(processor.gating2id)\n",
        "config.proj_dim = None\n",
        "\n",
        "model = BertForPreTraining('dsksd/bert-ko-small-minimal', config=config)"
      ],
      "id": "GIwp1qRD0bzC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/kyumin/Development/bc_dst/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4fbm9Pf0bzD"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ‚úÖ [MASK] Í≥ºÏ†ïÏùÄ Huggingface collator Îî∞Î¶Ñ\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=args.mlm_prob\n",
        ")"
      ],
      "id": "t4fbm9Pf0bzD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DB56vA_0bzD"
      },
      "source": [
        "## Trainer Ï†ïÏùò"
      ],
      "id": "1DB56vA_0bzD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGng28A0bzD"
      },
      "source": [
        "n_epochs = 10\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./checkpoints',\n",
        "    learning_rate=4e-4,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=n_epochs,\n",
        "    per_gpu_train_batch_size=30, # ÏÑúÎ≤ÑÏóê ÎßûÍ≤å ÏÑ§Ï†ï\n",
        "    save_steps=2000,\n",
        "    save_total_limit=10, # Î©îÎ™®Î¶¨ ÏÉùÍ∞ÅÌï¥ÏÑú ÏïåÏïÑÏÑú Ï°∞Ï†à !\n",
        "    logging_steps=2000,\n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"epoch\",  # `epoch`: Evaluate every end of epoch. / mlmÌèâÍ∞ÄÎäî lossÎ°ú ÌïòÎäîÍ≤å Ìé∏Ìï® \n",
        ")\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=20, early_stopping_threshold=0.0001\n",
        ")\n",
        "trainer = Trainer(\n",
        "    callbacks=[early_stopping], # callbackÏÇ¨Ïö©ÏùÑ ÏúÑÌï¥ ÌïÑÏöîÌïú argument\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ],
      "id": "_VGng28A0bzD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuNbfFkb0bzE"
      },
      "source": [
        "## Pretraining"
      ],
      "id": "zuNbfFkb0bzE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx43lSpz0bzE",
        "outputId": "01352c9b-687a-4e49-b3e3-70e2d851c5be"
      },
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./checkpoints/final_bert_model\")"
      ],
      "id": "mx43lSpz0bzE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 36946.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5BGlYya0bzE"
      },
      "source": [
        ""
      ],
      "id": "U5BGlYya0bzE",
      "execution_count": null,
      "outputs": []
    }
  ]
}