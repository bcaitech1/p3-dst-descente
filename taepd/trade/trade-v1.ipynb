{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e75f7a-efa4-4f8f-ae2b-9f146df29ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install prettyprinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480165-e5e6-40b2-be60-06ca8e46e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3863f74a-cbc9-48f2-ac06-e8a61d6ea7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoConfig\n",
    "\n",
    "from data_utils import (YamlConfigManager, WOSDataset, get_examples_from_dialogues, load_dataset,\n",
    "                        set_seed, custom_to_mask, custom_get_examples_from_dialogues, custom_load_dataset)\n",
    "\n",
    "from evaluation import _evaluation\n",
    "from inference import inference\n",
    "from model import TRADE, masked_cross_entropy_for_value\n",
    "from preprocessor import TRADEPreprocessor\n",
    "from prettyprinter import cpprint\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea76b50d-a9c5-47cd-b2e7-95c3cc7e670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70e2376-5bc5-403e-8a7f-186098cc0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easydict.EasyDict({\n",
      "    'data_dir': '../input/data/train_dataset',\n",
      "    'model_dir': 'results',\n",
      "    'train_batch_size': 4,\n",
      "    'eval_batch_size': 8,\n",
      "    'learning_rate': 3e-05,\n",
      "    'adam_epsilon': 1e-08,\n",
      "    'max_grad_norm': 1.0,\n",
      "    'num_train_epochs': 30,\n",
      "    'warmup_ratio': 0.0,\n",
      "    'random_seed': 42,\n",
      "    'n_gate': 5,\n",
      "    'teacher_forcing_ratio': 0.5,\n",
      "    'model_name_or_path': 'dsksd/bert-ko-small-minimal',\n",
      "    'proj_dim': 'None',\n",
      "    'tag': ['trade'],\n",
      "    'use_kfold': False,\n",
      "    'num_k': 0,\n",
      "    'val_ratio': 0.1,\n",
      "    'scheduler': 'Linear',\n",
      "    'mask': True\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "cfg = YamlConfigManager('./config.yml', 'base').values\n",
    "cpprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ac08d8-410a-4892-b1c5-f511005bc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current learning rate\n",
    "def get_lr(scheduler):\n",
    "    return scheduler.get_last_lr()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebb526d-d6f0-4a16-bdfa-600edb5fb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1086/6300 [00:00<00:00, 10859.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k  8\n",
      "[ 149 4765 4335 2340 4677 6176 5227 6556 5449  657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [00:00<00:00, 8423.36it/s] \n",
      "100%|██████████| 700/700 [00:00<00:00, 2595.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# random seed 고정\n",
    "set_seed(cfg.random_seed)\n",
    "\n",
    "# Data Loading\n",
    "train_data_file = f\"{cfg.data_dir}/train_dials.json\"\n",
    "slot_meta = json.load(open(f\"{cfg.data_dir}/slot_meta.json\"))\n",
    "# train_data, dev_data, dev_labels = load_dataset(train_data_file, cfg.val_ratio)\n",
    "train_data, dev_data, dev_labels = custom_load_dataset(train_data_file, cfg.val_ratio, k=8)\n",
    "\n",
    "train_examples = custom_get_examples_from_dialogues(\n",
    "    train_data, user_first=False, dialogue_level=False\n",
    ")\n",
    "dev_examples = custom_get_examples_from_dialogues(\n",
    "    dev_data, user_first=False, dialogue_level=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99340d3-9a2c-4fa9-9061-915f61dd5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSTInputExample(guid='snowy-hat-8324:관광_식당_11-0', context_turns=[], current_turn=['', ' # ', '서울 중앙에 있는 박물관을 찾아주세요', ' * '], label=['관광-종류-박물관', '관광-지역-서울 중앙'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07631841-706c-4b78-933f-5d32bde5fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessor\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path)\n",
    "\n",
    "# Dealing with long texts The maximum sequence length of BERT is 512.\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512, n_gate=cfg.n_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69681250-741e-467b-9179-b799e53f5a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting Featrues\n",
    "# cpprint('Extracting Features...')\n",
    "# train_features = processor.sep_custom_convert_examples_to_features(train_examples)\n",
    "# dev_features = processor.sep_custom_convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60162a83-bd1c-4fe1-8270-406f11549317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 train data InputFeatur 저장\n",
    "# with open('custom_train_features.txt', 'wb') as f:\n",
    "#     pickle.dump(train_features, f)\n",
    "# with open('custom_dev_features.txt', 'wb') as f:\n",
    "#     pickle.dump(dev_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211b7774-b447-4d87-973d-2ecd124b4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 파일 사용\n",
    "with open('custom_train_features.txt', 'rb') as f:\n",
    "    train_features = pickle.load(f)\n",
    "with open('custom_dev_features.txt', 'rb') as f:\n",
    "    dev_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ac70ca-510c-40a5-b2bd-845a246ba6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slot Meta tokenizing for the decoder initial inputs\n",
    "tokenized_slot_meta = []\n",
    "for slot in slot_meta:\n",
    "    tokenized_slot_meta.append(\n",
    "        tokenizer.encode(slot.replace(\"-\", \" \"), add_special_tokens=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec392330-ac93-499b-9d20-15370366ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is initialized\n"
     ]
    }
   ],
   "source": [
    "# Model 선언\n",
    "config = AutoConfig.from_pretrained('dsksd/bert-ko-small-minimal')\n",
    "config.model_name_or_path = 'dsksd/bert-ko-small-minimal'\n",
    "config.n_gate = cfg.n_gate\n",
    "config.proj_dim = None\n",
    "\n",
    "model = TRADE(config, tokenized_slot_meta)\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model is initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126758aa-9a0e-4eff-9a95-b1f35ca00a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaepd\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">iconic-mountain-34</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/taepd/DST\" target=\"_blank\">https://wandb.ai/taepd/DST</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/taepd/DST/runs/1bydb7fq\" target=\"_blank\">https://wandb.ai/taepd/DST/runs/1bydb7fq</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/repo/taepd/trade/wandb/run-20210518_173757-1bydb7fq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1bydb7fq)</h1><iframe src=\"https://wandb.ai/taepd/DST/runs/1bydb7fq\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd118e85a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --wandb initialize with configuration\n",
    "wandb.init(project='DST', tags=cfg.tag, config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e51a0d-f3d2-4a71-8e58-37dbafbb4130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 46127\n",
      "# dev: 5118\n"
     ]
    }
   ],
   "source": [
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=cfg.train_batch_size,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    "    num_workers=4,  # num_worker = 4 * num_GPU\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(\"# train:\", len(train_data))\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(\n",
    "    dev_data,\n",
    "    batch_size=cfg.eval_batch_size,\n",
    "    sampler=dev_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(\"# dev:\", len(dev_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32a5559-d31a-4937-8825-997da97673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 및 Scheduler 선언\n",
    "n_epochs = cfg.num_train_epochs\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=cfg.learning_rate, eps=cfg.adam_epsilon)\n",
    "warmup_steps = int(t_total * cfg.warmup_ratio)\n",
    "# learning rate decreases linearly from the initial lr set in the optimizer to 0\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    ")\n",
    "teacher_forcing = cfg.teacher_forcing_ratio\n",
    "\n",
    "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
    "loss_fnc_2 = nn.CrossEntropyLoss()  # gating\n",
    "loss_fnc_pretrain = nn.CrossEntropyLoss()  # MLM pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdb7b28-a37c-49f3-ad41-8c949ab5ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장될 파일 위치 생성\n",
    "if not os.path.exists(f\"{cfg.model_dir}\"):\n",
    "    os.mkdir(f\"{cfg.model_dir}\")\n",
    "if not os.path.exists(f\"{cfg.model_dir}/{wandb.run.name}\"):\n",
    "    os.mkdir(f\"{cfg.model_dir}/{wandb.run.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef705d3-5aa4-44a8-a169-34f2c7afcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    vars(cfg),\n",
    "    open(f\"{cfg.model_dir}/{wandb.run.name}/exp_config.json\", \"w\"),\n",
    "    indent=2,\n",
    "    ensure_ascii=False,\n",
    ")\n",
    "json.dump(\n",
    "    slot_meta,\n",
    "    open(f\"{cfg.model_dir}/slot_meta.json\", \"w\"),\n",
    "    indent=2,\n",
    "    ensure_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeda079-8a8c-4d60-93bc-bc84d376c6f3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0db76-2c8d-4e4d-98cc-33b2f29be7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# backward pass시 gradient 정보가 손실되지 않게 하려고 사용(loss에 scale factor를 곱해서 gradient 값이 너무 작아지는 것을 방지)\n",
    "scaler = GradScaler()\n",
    "best_score, best_checkpoint = 0, 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [\n",
    "            b.to(device) if not isinstance(b, list) else b for b in batch\n",
    "        ]\n",
    "        # mask\n",
    "        if cfg.mask:\n",
    "            change_mask_prop = 0.8\n",
    "            mask_p = random.random()\n",
    "            if cfg.mask and mask_p < change_mask_prop:\n",
    "                input_ids = custom_to_mask(input_ids)\n",
    "        # teacher forcing\n",
    "        if (\n",
    "            teacher_forcing > 0.0\n",
    "            and random.random() < teacher_forcing\n",
    "        ):\n",
    "            tf = target_ids\n",
    "        else:\n",
    "            tf = None\n",
    "\n",
    "        optimizer.zero_grad()  # optimizer는 input으로 model parameter를 가진다 -> zero_grad()로 파라미터 컨드롤 가능\n",
    "\n",
    "        with autocast():  # 밑에 해당하는 코드를 자동으로 mixed precision으로 변환시켜서 실행\n",
    "            all_point_outputs, all_gate_outputs = model(\n",
    "                input_ids, segment_ids, input_masks, target_ids.size(-1), tf\n",
    "            )\n",
    "            # generation loss\n",
    "            loss_1 = loss_fnc_1(\n",
    "                all_point_outputs.contiguous(),\n",
    "                target_ids.contiguous().view(-1),\n",
    "                tokenizer.pad_token_id,\n",
    "            )\n",
    "            # gating loss\n",
    "            loss_2 = loss_fnc_2(\n",
    "                all_gate_outputs.contiguous().view(-1, cfg.n_gate),\n",
    "                gating_ids.contiguous().view(-1),\n",
    "            )\n",
    "            loss = loss_1 + loss_2\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # global_step 추가 부분\n",
    "        wandb.log({\"train/learning_rate\": get_lr(scheduler),\n",
    "                   \"train/epoch\": epoch\n",
    "                   })\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"[{epoch}/{n_epochs}] [{step}/{len(train_loader)}] loss: {loss.item()} gen: {loss_1.item()} gate: {loss_2.item()}\"\n",
    "            )\n",
    "\n",
    "            # -- train 단계에서 Loss, Accuracy 로그 저장\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/gen_loss\": loss_1.item(),\n",
    "                \"train/gate_loss\": loss_2.item(),\n",
    "            })\n",
    "\n",
    "    predictions, p_logits, g_logits = inference(model, dev_loader, processor, device, cfg.n_gate)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "\n",
    "    # -- eval 단계에서 Loss, Accuracy 로그 저장\n",
    "    wandb.log({\n",
    "        \"eval/join_goal_acc\": eval_result[\"joint_goal_accuracy\"],\n",
    "        \"eval/turn_slot_f1\": eval_result[\"turn_slot_f1\"],\n",
    "        \"eval/turn_slot_acc\": eval_result[\"turn_slot_accuracy\"],\n",
    "    })\n",
    "\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    if best_score < eval_result['joint_goal_accuracy']:\n",
    "        cpprint(f\"--Update Best checkpoint!, epoch: {epoch+1}\")\n",
    "        best_score = eval_result['joint_goal_accuracy']\n",
    "        best_checkpoint = epoch\n",
    "        if not os.path.isdir(cfg.model_dir):\n",
    "            os.makedirs(cfg.model_dir)\n",
    "        print(\"--Saving best model checkpoint\")\n",
    "        torch.save(model.state_dict(), f\"{cfg.model_dir}/{wandb.run.name}/best.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler.state_dict': scheduler.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'gen_loss': loss_1.item(),\n",
    "            'gate_loss': loss_2.item(),\n",
    "        }, os.path.join(f\"{cfg.model_dir}/{wandb.run.name}\", \"training_best_checkpoint.bin\"))\n",
    "        \n",
    "        # logit 저장\n",
    "        np.save(os.path.join(f\"{cfg.model_dir}/{wandb.run.name}\", r'p_logits.npy'), p_logits)\n",
    "        np.save(os.path.join(f\"{cfg.model_dir}/{wandb.run.name}\", r'g_logits.npy'), g_logits)\n",
    "\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"{cfg.model_dir}/{wandb.run.name}/last.pth\")\n",
    "    print(f\"time for 1 epoch: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0c971-fe9d-42af-8bcc-6d3960735c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9db0a5-d2b1-4809-bae5-01b14bcba1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
